# BenchmarkBot

Create low, medium, and hard benchmark in every possible field you can think of.
Test it aganist the top performing AI.

Field Research and Innovation:
Give all the data upto certain certain year, ask two or more AI Agents to come with innovative ideas to move the field forward.
For example, give all information until 1900, ask the AI agents to move the field forward, to discover something like Special thoery of Relativity. 

- A benchmark that evaulates trustworthiness of the AGI/Agents.
- A benchmark that evaluates the how much the response of the system is influenced by external or internal factors.
- A benchmark  on reliability test
- In coding, benchmark to test the consistent, error free code generation on general task or specific task
- End user experience test
- Complex task handling benchmarks for llm
- Adjust temperature and other parameters dynamically based on the task
- Adaptability benchmark of llm on task specific
- Benchmarks on creation of new tool/components for a project or in general
- Edge case handling benchmarks
- Security and vulnerability benchmarks

Create a system where anyone can run the benchmark without much difficulty.
Just running a small script or package should give the results.
- Aim is to ensure time to time checking of the model performance aganist the original posted benchmark on runtime. 

Benchmark the reasoning steps of the llms
- Evaluate the entire reasoning steps of the llms rather than only final answers
- Split into different categories like coding, solving a logical problems,... so on
- Evaluate human reasoning and the LLM reasoning. Categories humans in different levels of intelligence, someone who is beginner, medium and expert. or split into different levels. 

Bug free code generation benchmark 

User expected code generation benchmark (Understanding/Interpretation of the user query)

Test case generation benchmarks for both open source and closed source solutions 

Benchmark on generalisation capabilities/learning capabilities 

**A environment/site that host the evaluation of all the available open and closed source solutions for any type of problem the framework provides solutions to**
Extend it to other domains such as E.g Robotics,...

- AI agents benchmark
- Idea generation/Creativity benchmark
- A benchmark that evaluates the ads in the LLM output and output information based corporate interests rather than facts, monetisation monitoring, both direct and indirect and all forms of ads, and keep track of these
- Benchmark deep research results, also applicability to drive innovation in research by companies/institutions and in academic settings 
- Realistic(Positive & Negative) response benchmarks rather than focusing only on positive response.
- Benchmark the context findings from image.
- Benchmark the correctness of llm output. Maybe from the reference it provides.