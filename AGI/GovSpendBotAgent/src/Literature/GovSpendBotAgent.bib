@Article{Fang2024,
  author   = {Fang, Xi and Xu, Weijie and Tan, Fiona Anting and Zhang, Jiani and Hu, Ziqing and Qi, Yanjun Jane and Nickleach, Scott and Socolinsky, Diego and Sengamedu, Srinivasan and Faloutsos, Christos and others},
  title    = {Large language models (LLMs) on tabular data: Prediction, generation, and understanding-a survey},
  year     = {2024},
  abstract = {Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.},
  comment  = {Citation -27},
  ranking  = {rank5},
  url      = {https://www.amazon.science/publications/large-language-models-llms-on-tabular-data-prediction-generation-and-understanding-a-survey},
}

@Misc{HongWei2024,
  author   = {Hong-Wei, Wu},
  note     = {Accessed: 2024-05-30},
  title    = {Awesome-LLM-Tabular},
  year     = {2024},
  abstract = {Since the emergence of ChatGPT, Large Language Models (LLMs) have garnered significant attention, with new advancements continuously emerging. LLMs have found applications in various domains like vision, audio, and text tasks. However, tabular data remains a crucial data format in this world. Hence, this repo focuses on collecting research papers that explore the integration of LLM technology with tabular data, and aims to save you valuable time and boost research efficiency.

Awesome-LLM-Tabular is a curated list of Large Language Model applied to Tabular Data.

This project is currently under development. Feel free to ‚≠ê (STAR) and üî≠ (WATCH) it to stay updated on the latest developments.},
  orcid    = {https://orcid.org/0009-0005-8073-5297},
  url      = {https://github.com/johnnyhwu/Awesome-LLM-Tabular},
}

@Misc{Lu,
  author       = {Weizheng Lu},
  howpublished = {\url{https://github.com/godaai/llm-table-survey}},
  note         = {[Accessed 12-10-2024]},
  title        = {GitHub - llm-table-survey},
  abstract     = {Survey on Tabular LLM},
  url          = {https://github.com/godaai/llm-table-survey},
}

@InProceedings{Li2023,
  author    = {Li, Hongxin and Su, Jingran and Chen, Yuntao and Li, Qing and ZHANG, ZHAO-XIANG},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {{SheetCopilot}: {Bringing} {Software} {Productivity} to the {Next} {Level} through {Large} {Language} {Models}},
  year      = {2023},
  editor    = {Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  pages     = {4952--4984},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0ff30c4bf31db0119a6219e0d250e037-Paper-Conference.pdf},
}

@Article{Zhang2024,
  author    = {Zhang, Weixu and Wang, Yifei and Song, Yuanfeng and Wei, Victor Junqiu and Tian, Yuxing and Qi, Yiyan and Chan, Jonathan H and Wong, Raymond Chi-Wing and Yang, Haiqin},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  title     = {Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey},
  year      = {2024},
  abstract  = {The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.},
  comment   = {Citations - 6 (12/10/2024)},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/2310.17894},
}

@InProceedings{Zhao2024,
  author    = {Zhao, Yilun and Long, Yitao and Liu, Hongjun and Kamoi, Ryo and Nan, Linyong and Chen, Lyuhao and Liu, Yixin and Tang, Xiangru and Zhang, Rui and Cohan, Arman},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Financial Documents},
  year      = {2024},
  pages     = {16103--16120},
  abstract  = {Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing financial documents containing both text and tables. We evaluate a wide spectrum of 27 LLMs, including those specialized in math, coding and finance, with Chain-of-Thought and Program-of-Thought prompting methods. We found that even the current best-performing system (ie, GPT-4) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe DocMath-Eval can be used as a valuable benchmark to evaluate LLMs‚Äô capabilities to solve challenging numerical reasoning problems in expert domains.},
  comment   = {Citations - 1 (12/10/2024)},
  url       = {https://aclanthology.org/2024.acl-long.852.pdf},
}

@Article{Sui2023,
  author   = {Sui, Yuan and Zou, Jiaru and Zhou, Mengyu and He, Xinyi and Du, Lun and Han, Shi and Zhang, Dongmei},
  journal  = {arXiv preprint arXiv:2312.09039},
  title    = {Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning},
  year     = {2023},
  abstract = {Table reasoning tasks have shown remarkable progress with the development of large language models (LLMs), which involve interpreting and drawing conclusions from tabular data based on natural language (NL) questions. Existing solutions mainly tested on smaller tables face scalability issues and struggle with complex queries due to incomplete or dispersed data across different table sections. To alleviate
these challenges, we propose TAP4LLM as a versatile pre-processor suite for leveraging LLMs in table-based tasks effectively. It covers several distinct components: (1) table sampling to decompose large tables into manageable subtables based on query semantics, (2) table augmentation to enhance tables with additional knowledge from external sources or models, and (3) table packing & serialization to convert tables into various formats suitable for LLMs‚Äô understanding. In each module, we design and
compare several common methods under various usage scenarios, aiming to shed light on the
best practices for leveraging LLMs for tablereasoning tasks. Our experiments show that
our method improves LLMs‚Äô reasoning capabilities in various tabular tasks and enhances
the interaction between LLMs and tabular data
by employing effective pre-processing.},
  comment  = {Citations - 10 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2312.09039},
}

@InProceedings{Wang2021,
  author    = {Wang, Zhiruo and Dong, Haoyu and Jia, Ran and Li, Jia and Fu, Zhiyi and Han, Shi and Zhang, Dongmei},
  booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  title     = {TUTA: Tree-based Transformers for Generally Structured Table Pre-training},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {1780‚Äì1790},
  publisher = {Association for Computing Machinery},
  series    = {KDD '21},
  abstract  = {We propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets.},
  doi       = {10.1145/3447548.3467434},
  isbn      = {9781450383325},
  keywords  = {transformer, self supervision, generally structured table},
  location  = {Virtual Event, Singapore},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3447548.3467434},
}

@Article{Deng2022,
  author     = {Deng, Xiang and Sun, Huan and Lees, Alyssa and Wu, You and Yu, Cong},
  journal    = {SIGMOD Rec.},
  title      = {TURL: Table Understanding through Representation Learning},
  year       = {2022},
  issn       = {0163-5808},
  month      = jun,
  number     = {1},
  pages      = {33‚Äì40},
  volume     = {51},
  abstract   = {Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in a self-supervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning.},
  address    = {New York, NY, USA},
  comment    = {Citations - 29 (13/10/2024)},
  doi        = {10.1145/3542700.3542709},
  issue_date = {March 2022},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3542700.3542709},
}

@Article{Yin2020,
  author   = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
  journal  = {arXiv preprint arXiv:2005.08314},
  title    = {TaBERT: Pretraining for joint understanding of textual and tabular data},
  year     = {2020},
  abstract = {Recent years have witnessed the burgeoning of pretrained language models (LMs) for textbased natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both
free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TABERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TABERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TABERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WIKITABLEQUESTIONS,
while performing competitively on the text-toSQL dataset SPIDER.},
  comment  = {Citations -541 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2005.08314},
}

@InProceedings{Herzig2020,
  author    = {Herzig, Jonathan and Nowak, Pawel Krzysztof and M{\"u}ller, Thomas and Piccinno, Francesco and Eisenschlos, Julian},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  title     = {{T}a{P}as: Weakly Supervised Table Parsing via Pre-training},
  year      = {2020},
  address   = {Online},
  editor    = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  month     = jul,
  pages     = {4320--4333},
  publisher = {Association for Computational Linguistics},
  abstract  = {Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT{'}s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.},
  comment   = {Citations - 599(13/10/2024)},
  doi       = {10.18653/v1/2020.acl-main.398},
  url       = {https://aclanthology.org/2020.acl-main.398},
}

@InProceedings{Iida2021,
  author    = {Iida, Hiroshi and Thai, Dung and Manjunatha, Varun and Iyyer, Mohit},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {{TABBIE}: Pretrained Representations of Tabular Data},
  year      = {2021},
  address   = {Online},
  editor    = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  month     = jun,
  pages     = {3446--3456},
  publisher = {Association for Computational Linguistics},
  abstract  = {Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model{'}s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.},
  comment   = {Citations -},
  doi       = {10.18653/v1/2021.naacl-main.270},
  url       = {https://aclanthology.org/2021.naacl-main.270},
}

@Article{Pasupat2015,
  author   = {Pasupat, Panupong and Liang, Percy},
  journal  = {arXiv preprint arXiv:1508.00305},
  title    = {Compositional semantic parsing on semi-structured tables},
  year     = {2015},
  abstract = {Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering
complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and
show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.},
  comment  = {Citations - 723 (13/101/2024)},
  url      = {https://arxiv.org/pdf/1508.00305},
}

@InProceedings{Jin2022,
  author       = {Jin, Nengzheng and Siebert, Joanna and Li, Dongfang and Chen, Qingcai},
  booktitle    = {China Conference on Knowledge Graph and Semantic Computing},
  title        = {A survey on table question answering: recent advances},
  year         = {2022},
  organization = {Springer},
  pages        = {174--186},
  abstract     = {Table Question Answering (Table QA) refers to providing precise answers from tables to answer a user‚Äôs question. In recent years, there have been a lot of works on table QA, but there is a lack of comprehensive surveys on this research topic. Hence, we aim to provide an overview of available datasets and representative methods in table QA. We classify existing methods for table QA into five categories according to their techniques, which include semantic-parsing-based, generative,
extractive, matching-based, and retriever-reader-based methods. Moreover, because table QA is still a challenging task for existing methods, we also identify and outline several key challenges and discuss the potential future directions of table QA.},
  comment      = {Citations -37 (13/10/2024)},
  url          = {https://arxiv.org/pdf/2207.05270},
}

@Article{Zhong2017,
  author   = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  journal  = {arXiv preprint arXiv:1709.00103},
  title    = {Seq2sql: Generating structured queries from natural language using reinforcement learning},
  year     = {2017},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.},
  comment  = {Citations - 1155 (13/10/2024)},
  url      = {https://arxiv.org/pdf/1709.00103},
}

@Article{Xu2017,
  author   = {Xu, Xiaojun and Liu, Chang and Song, Dawn},
  journal  = {arXiv preprint arXiv:1711.04436},
  title    = {Sqlnet: Generating structured queries from natural language without reinforcement learning},
  year     = {2017},
  abstract = {Synthesizing SQL queries from natural language is a long-standing open problem and has been attracting considerable interest recently. Toward solving the problem, the de facto approach is to employ a sequence-to-sequence-style model. Such an approach will necessarily require the SQL queries to be serialized. Since the same SQL query may have multiple equivalent serializations, training a sequence-to-sequence-style model is sensitive to the choice from one of them. This phenomenon is documented as the "order-matters" problem. Existing state-of-the-art approaches rely on reinforcement learning to reward the decoder when it generates any of the equivalent serializations. However, we observe that the improvement from reinforcement learning is limited. In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally solve this problem by avoiding the sequence-to-sequence structure when the order does not matter. In particular, we employ a sketch-based approach where the sketch contains a dependency graph so that one prediction can be done by taking into consideration only the previous predictions that it depends on. In addition, we propose a sequence-to-set model as well as the column attention mechanism to synthesize the query based on the sketch. By combining all these novel techniques, we show that SQLNet can outperform the prior art by 9% to 13% on the WikiSQL task.},
  comment  = {Citations - 431 (13/10/2024)},
  url      = {https://arxiv.org/pdf/1711.04436},
}

@Article{Yu2018,
  author   = {Yu, Tao and Li, Zifan and Zhang, Zilin and Zhang, Rui and Radev, Dragomir},
  journal  = {arXiv preprint arXiv:1804.09769},
  title    = {Typesql: Knowledge-based type-aware neural text-to-sql generation},
  year     = {2018},
  abstract = {Interacting with relational databases through natural language helps users of any background easily query and analyze a vast amount of data. This requires a system that understands users' questions and converts them to SQL queries automatically. In this paper we present a novel approach, TypeSQL, which views this problem as a slot filling task. Additionally, TypeSQL utilizes type information to better understand rare entities and numbers in natural language questions. We test this idea on the WikiSQL dataset and outperform the prior state-of-the-art by 5.5% in much less time. We also show that accessing the content of databases can significantly improve the performance when users' queries are not well-formed. TypeSQL gets 82.6% accuracy, a 17.5% absolute improvement compared to the previous content-sensitive model.},
  comment  = {Citations - 295 (13/10/2024)},
  url      = {https://arxiv.org/pdf/1804.09769},
}

@Article{Abraham2022,
  author   = {Abraham, Abhijith Neil and Rahman, Fariz and Kaur, Damanpreet},
  journal  = {arXiv preprint arXiv:2202.00454},
  title    = {Tablequery: Querying tabular data with natural language},
  year     = {2022},
  abstract = {This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery.},
  comment  = {Citations - 2 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2202.00454},
}

@Article{Wei2022,
  author   = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal  = {Advances in neural information processing systems},
  title    = {Chain-of-thought prompting elicits reasoning in large language models},
  year     = {2022},
  pages    = {24824--24837},
  volume   = {35},
  abstract = {We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  comment  = {Citations - 7741 (14/10/2024)},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
}

@Article{Zhou2023,
  author   = {Zhou, Houquan and Hou, Yang and Li, Zhenghua and Wang, Xuebin and Wang, Zhefeng and Duan, Xinyu and Zhang, Min},
  journal  = {arXiv preprint arXiv:2311.08287},
  title    = {How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions},
  year     = {2023},
  abstract = {While recent advancements in large language models (LLMs) bring us closer to achieving artificial general intelligence, the question persists: Do LLMs truly understand language, or do they merely mimic comprehension through pattern recognition? This study seeks to explore this question through the lens of syntax, a crucial component of sentence comprehension. Adopting a natural language question-answering (Q&A) scheme, we craft questions targeting nine syntactic knowledge points that are most closely related to sentence comprehension. Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points. In particular, questions involving prepositional phrase attachment pose the greatest challenge, whereas those concerning adjectival modifier and indirect object are relatively easier for LLMs to handle. Furthermore, a case study on the training dynamics of the LLMs reveals that the majority of syntactic knowledge is learned during the initial stages of training, hinting that simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLMs.},
  comment  = {Citations - 1(14/10/2024)},
  url      = {https://arxiv.org/pdf/2311.08287},
}

@Article{Jaitly2023,
  author   = {Jaitly, Sukriti and Shah, Tanay and Shugani, Ashish and Grewal, Razik Singh},
  journal  = {arXiv preprint arXiv:2312.12464},
  title    = {Towards Better Serialization of Tabular Data for Few-shot Classification},
  year     = {2023},
  abstract = {We present a study on the integration of Large Language Models (LLMs) in tabular data classification, emphasizing an efficient framework. Building upon existing work done in TabLLM (arXiv:2210.10723), we introduce three novel serialization techniques, including the standout LaTeX serialization method. This method significantly boosts the performance of LLMs in processing domain-specific datasets, Our method stands out for its memory efficiency and ability to fully utilize complex data structures. Through extensive experimentation, including various serialization approaches like feature combination and importance, we demonstrate our work's superiority in accuracy and efficiency over traditional models.},
  comment  = {Citations - 2 (14/10/2024)},
  url      = {https://arxiv.org/pdf/2312.12464},
}

@Article{Wang2024,
  author   = {Wang, Zilong and Zhang, Hao and Li, Chun-Liang and Eisenschlos, Julian Martin and Perot, Vincent and Wang, Zifeng and Miculicich, Lesly and Fujii, Yasuhisa and Shang, Jingbo and Lee, Chen-Yu and others},
  journal  = {arXiv preprint arXiv:2401.04398},
  title    = {Chain-of-table: Evolving tables in the reasoning chain for table understanding},
  year     = {2024},
  abstract = {Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the CHAIN-OF-TABLE framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the
intermediate results, enabling more accurate and reliable predictions. CHAINOF-TABLE achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.},
  comment  = {Citations - 34 (14/10/2024)},
  url      = {https://arxiv.org/pdf/2401.04398},
}

@Article{Cheng2022,
  author   = {Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and others},
  journal  = {arXiv preprint arXiv:2210.02875},
  title    = {Binding language models in symbolic languages},
  year     = {2022},
  abstract = {Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .},
  comment  = {Citations - 139 (15/10/2024)},
  url      = {https://arxiv.org/pdf/2210.02875},
}

@Article{Liu2023,
  author   = {Liu, Tianyang and Wang, Fei and Chen, Muhao},
  journal  = {arXiv preprint arXiv:2312.16702},
  title    = {Rethinking Tabular Data Understanding with Large Language Models},
  year     = {2023},
  abstract = {Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WIKITABLEQUESTIONS, representing a substantial advancement over previous existing table processing paradigms of LLMs.},
  comment  = {Citations - 9 (15/10/2024)},
  url      = {https://arxiv.org/pdf/2312.16702},
}

@Article{Ruan2024,
  author   = {Ruan, Yucheng and Lan, Xiang and Ma, Jingying and Dong, Yizhi and He, Kai and Feng, Mengling},
  journal  = {arXiv preprint arXiv:2408.10548},
  title    = {Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution},
  year     = {2024},
  abstract = {Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.},
  comment  = {Citations - 0 (15/10/2024)},
  url      = {https://arxiv.org/pdf/2408.10548},
}

@Comment{jabref-meta: databaseType:bibtex;}
