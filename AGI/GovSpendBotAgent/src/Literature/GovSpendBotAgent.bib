@Article{Fang2024,
  author   = {Fang, Xi and Xu, Weijie and Tan, Fiona Anting and Zhang, Jiani and Hu, Ziqing and Qi, Yanjun Jane and Nickleach, Scott and Socolinsky, Diego and Sengamedu, Srinivasan and Faloutsos, Christos and others},
  title    = {Large language models (LLMs) on tabular data: Prediction, generation, and understanding-a survey},
  year     = {2024},
  abstract = {Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.},
  comment  = {Citation -27},
  ranking  = {rank5},
  url      = {https://www.amazon.science/publications/large-language-models-llms-on-tabular-data-prediction-generation-and-understanding-a-survey},
}

@Misc{HongWei2024,
  author   = {Hong-Wei, Wu},
  note     = {Accessed: 2024-05-30},
  title    = {Awesome-LLM-Tabular},
  year     = {2024},
  abstract = {Since the emergence of ChatGPT, Large Language Models (LLMs) have garnered significant attention, with new advancements continuously emerging. LLMs have found applications in various domains like vision, audio, and text tasks. However, tabular data remains a crucial data format in this world. Hence, this repo focuses on collecting research papers that explore the integration of LLM technology with tabular data, and aims to save you valuable time and boost research efficiency.

Awesome-LLM-Tabular is a curated list of Large Language Model applied to Tabular Data.

This project is currently under development. Feel free to ‚≠ê (STAR) and üî≠ (WATCH) it to stay updated on the latest developments.},
  orcid    = {https://orcid.org/0009-0005-8073-5297},
  url      = {https://github.com/johnnyhwu/Awesome-LLM-Tabular},
}

@Misc{Lu,
  author       = {Weizheng Lu},
  howpublished = {\url{https://github.com/godaai/llm-table-survey}},
  note         = {[Accessed 12-10-2024]},
  title        = {GitHub - llm-table-survey},
  abstract     = {Survey on Tabular LLM},
  url          = {https://github.com/godaai/llm-table-survey},
}

@InProceedings{Li2023,
  author    = {Li, Hongxin and Su, Jingran and Chen, Yuntao and Li, Qing and ZHANG, ZHAO-XIANG},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {{SheetCopilot}: {Bringing} {Software} {Productivity} to the {Next} {Level} through {Large} {Language} {Models}},
  year      = {2023},
  editor    = {Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  pages     = {4952--4984},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0ff30c4bf31db0119a6219e0d250e037-Paper-Conference.pdf},
}

@Article{Zhang2024,
  author    = {Zhang, Weixu and Wang, Yifei and Song, Yuanfeng and Wei, Victor Junqiu and Tian, Yuxing and Qi, Yiyan and Chan, Jonathan H and Wong, Raymond Chi-Wing and Yang, Haiqin},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  title     = {Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey},
  year      = {2024},
  abstract  = {The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.},
  comment   = {Citations - 6 (12/10/2024)},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/2310.17894},
}

@InProceedings{Zhao2024,
  author    = {Zhao, Yilun and Long, Yitao and Liu, Hongjun and Kamoi, Ryo and Nan, Linyong and Chen, Lyuhao and Liu, Yixin and Tang, Xiangru and Zhang, Rui and Cohan, Arman},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Financial Documents},
  year      = {2024},
  pages     = {16103--16120},
  abstract  = {Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing financial documents containing both text and tables. We evaluate a wide spectrum of 27 LLMs, including those specialized in math, coding and finance, with Chain-of-Thought and Program-of-Thought prompting methods. We found that even the current best-performing system (ie, GPT-4) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe DocMath-Eval can be used as a valuable benchmark to evaluate LLMs‚Äô capabilities to solve challenging numerical reasoning problems in expert domains.},
  comment   = {Citations - 1 (12/10/2024)},
  url       = {https://aclanthology.org/2024.acl-long.852.pdf},
}

@Article{Sui2023,
  author   = {Sui, Yuan and Zou, Jiaru and Zhou, Mengyu and He, Xinyi and Du, Lun and Han, Shi and Zhang, Dongmei},
  journal  = {arXiv preprint arXiv:2312.09039},
  title    = {Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning},
  year     = {2023},
  abstract = {Table reasoning tasks have shown remarkable progress with the development of large language models (LLMs), which involve interpreting and drawing conclusions from tabular data based on natural language (NL) questions. Existing solutions mainly tested on smaller tables face scalability issues and struggle with complex queries due to incomplete or dispersed data across different table sections. To alleviate
these challenges, we propose TAP4LLM as a versatile pre-processor suite for leveraging LLMs in table-based tasks effectively. It covers several distinct components: (1) table sampling to decompose large tables into manageable subtables based on query semantics, (2) table augmentation to enhance tables with additional knowledge from external sources or models, and (3) table packing & serialization to convert tables into various formats suitable for LLMs‚Äô understanding. In each module, we design and
compare several common methods under various usage scenarios, aiming to shed light on the
best practices for leveraging LLMs for tablereasoning tasks. Our experiments show that
our method improves LLMs‚Äô reasoning capabilities in various tabular tasks and enhances
the interaction between LLMs and tabular data
by employing effective pre-processing.},
  comment  = {Citations - 10 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2312.09039},
}

@Comment{jabref-meta: databaseType:bibtex;}
