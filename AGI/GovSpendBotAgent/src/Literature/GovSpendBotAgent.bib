@Article{Fang2024,
  author   = {Fang, Xi and Xu, Weijie and Tan, Fiona Anting and Zhang, Jiani and Hu, Ziqing and Qi, Yanjun Jane and Nickleach, Scott and Socolinsky, Diego and Sengamedu, Srinivasan and Faloutsos, Christos and others},
  title    = {Large language models (LLMs) on tabular data: Prediction, generation, and understanding-a survey},
  year     = {2024},
  abstract = {Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.},
  comment  = {Citation -27},
  ranking  = {rank5},
  url      = {https://www.amazon.science/publications/large-language-models-llms-on-tabular-data-prediction-generation-and-understanding-a-survey},
}

@Misc{HongWei2024,
  author   = {Hong-Wei, Wu},
  note     = {Accessed: 2024-05-30},
  title    = {Awesome-LLM-Tabular},
  year     = {2024},
  abstract = {Since the emergence of ChatGPT, Large Language Models (LLMs) have garnered significant attention, with new advancements continuously emerging. LLMs have found applications in various domains like vision, audio, and text tasks. However, tabular data remains a crucial data format in this world. Hence, this repo focuses on collecting research papers that explore the integration of LLM technology with tabular data, and aims to save you valuable time and boost research efficiency.

Awesome-LLM-Tabular is a curated list of Large Language Model applied to Tabular Data.

This project is currently under development. Feel free to ‚≠ê (STAR) and üî≠ (WATCH) it to stay updated on the latest developments.},
  orcid    = {https://orcid.org/0009-0005-8073-5297},
  url      = {https://github.com/johnnyhwu/Awesome-LLM-Tabular},
}

@Misc{Lu,
  author       = {Weizheng Lu},
  howpublished = {\url{https://github.com/godaai/llm-table-survey}},
  note         = {[Accessed 12-10-2024]},
  title        = {GitHub - llm-table-survey},
  abstract     = {Survey on Tabular LLM},
  url          = {https://github.com/godaai/llm-table-survey},
}

@InProceedings{Li2023,
  author    = {Li, Hongxin and Su, Jingran and Chen, Yuntao and Li, Qing and ZHANG, ZHAO-XIANG},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {{SheetCopilot}: {Bringing} {Software} {Productivity} to the {Next} {Level} through {Large} {Language} {Models}},
  year      = {2023},
  editor    = {Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  pages     = {4952--4984},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0ff30c4bf31db0119a6219e0d250e037-Paper-Conference.pdf},
}

@Article{Zhang2024,
  author    = {Zhang, Weixu and Wang, Yifei and Song, Yuanfeng and Wei, Victor Junqiu and Tian, Yuxing and Qi, Yiyan and Chan, Jonathan H and Wong, Raymond Chi-Wing and Yang, Haiqin},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  title     = {Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey},
  year      = {2024},
  abstract  = {The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.},
  comment   = {Citations - 6 (12/10/2024)},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/2310.17894},
}

@InProceedings{Zhao2024,
  author    = {Zhao, Yilun and Long, Yitao and Liu, Hongjun and Kamoi, Ryo and Nan, Linyong and Chen, Lyuhao and Liu, Yixin and Tang, Xiangru and Zhang, Rui and Cohan, Arman},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Financial Documents},
  year      = {2024},
  pages     = {16103--16120},
  abstract  = {Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing financial documents containing both text and tables. We evaluate a wide spectrum of 27 LLMs, including those specialized in math, coding and finance, with Chain-of-Thought and Program-of-Thought prompting methods. We found that even the current best-performing system (ie, GPT-4) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe DocMath-Eval can be used as a valuable benchmark to evaluate LLMs‚Äô capabilities to solve challenging numerical reasoning problems in expert domains.},
  comment   = {Citations - 1 (12/10/2024)},
  url       = {https://aclanthology.org/2024.acl-long.852.pdf},
}

@Article{Sui2023,
  author   = {Sui, Yuan and Zou, Jiaru and Zhou, Mengyu and He, Xinyi and Du, Lun and Han, Shi and Zhang, Dongmei},
  journal  = {arXiv preprint arXiv:2312.09039},
  title    = {Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning},
  year     = {2023},
  abstract = {Table reasoning tasks have shown remarkable progress with the development of large language models (LLMs), which involve interpreting and drawing conclusions from tabular data based on natural language (NL) questions. Existing solutions mainly tested on smaller tables face scalability issues and struggle with complex queries due to incomplete or dispersed data across different table sections. To alleviate
these challenges, we propose TAP4LLM as a versatile pre-processor suite for leveraging LLMs in table-based tasks effectively. It covers several distinct components: (1) table sampling to decompose large tables into manageable subtables based on query semantics, (2) table augmentation to enhance tables with additional knowledge from external sources or models, and (3) table packing & serialization to convert tables into various formats suitable for LLMs‚Äô understanding. In each module, we design and
compare several common methods under various usage scenarios, aiming to shed light on the
best practices for leveraging LLMs for tablereasoning tasks. Our experiments show that
our method improves LLMs‚Äô reasoning capabilities in various tabular tasks and enhances
the interaction between LLMs and tabular data
by employing effective pre-processing.},
  comment  = {Citations - 10 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2312.09039},
}

@InProceedings{Wang2021,
  author    = {Wang, Zhiruo and Dong, Haoyu and Jia, Ran and Li, Jia and Fu, Zhiyi and Han, Shi and Zhang, Dongmei},
  booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  title     = {TUTA: Tree-based Transformers for Generally Structured Table Pre-training},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {1780‚Äì1790},
  publisher = {Association for Computing Machinery},
  series    = {KDD '21},
  abstract  = {We propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets.},
  doi       = {10.1145/3447548.3467434},
  isbn      = {9781450383325},
  keywords  = {transformer, self supervision, generally structured table},
  location  = {Virtual Event, Singapore},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3447548.3467434},
}

@Article{Deng2022,
  author     = {Deng, Xiang and Sun, Huan and Lees, Alyssa and Wu, You and Yu, Cong},
  journal    = {SIGMOD Rec.},
  title      = {TURL: Table Understanding through Representation Learning},
  year       = {2022},
  issn       = {0163-5808},
  month      = jun,
  number     = {1},
  pages      = {33‚Äì40},
  volume     = {51},
  abstract   = {Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in a self-supervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning.},
  address    = {New York, NY, USA},
  comment    = {Citations - 29 (13/10/2024)},
  doi        = {10.1145/3542700.3542709},
  issue_date = {March 2022},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3542700.3542709},
}

@Article{Yin2020,
  author   = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
  journal  = {arXiv preprint arXiv:2005.08314},
  title    = {TaBERT: Pretraining for joint understanding of textual and tabular data},
  year     = {2020},
  abstract = {Recent years have witnessed the burgeoning of pretrained language models (LMs) for textbased natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both
free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TABERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TABERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TABERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WIKITABLEQUESTIONS,
while performing competitively on the text-toSQL dataset SPIDER.},
  comment  = {Citations -541 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2005.08314},
}

@InProceedings{Herzig2020,
  author    = {Herzig, Jonathan and Nowak, Pawel Krzysztof and M{\"u}ller, Thomas and Piccinno, Francesco and Eisenschlos, Julian},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  title     = {{T}a{P}as: Weakly Supervised Table Parsing via Pre-training},
  year      = {2020},
  address   = {Online},
  editor    = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  month     = jul,
  pages     = {4320--4333},
  publisher = {Association for Computational Linguistics},
  abstract  = {Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT{'}s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.},
  comment   = {Citations - 599(13/10/2024)},
  doi       = {10.18653/v1/2020.acl-main.398},
  url       = {https://aclanthology.org/2020.acl-main.398},
}

@InProceedings{Iida2021,
  author    = {Iida, Hiroshi and Thai, Dung and Manjunatha, Varun and Iyyer, Mohit},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {{TABBIE}: Pretrained Representations of Tabular Data},
  year      = {2021},
  address   = {Online},
  editor    = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  month     = jun,
  pages     = {3446--3456},
  publisher = {Association for Computational Linguistics},
  abstract  = {Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model{'}s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.},
  comment   = {Citations -},
  doi       = {10.18653/v1/2021.naacl-main.270},
  url       = {https://aclanthology.org/2021.naacl-main.270},
}

@Article{Pasupat2015,
  author   = {Pasupat, Panupong and Liang, Percy},
  journal  = {arXiv preprint arXiv:1508.00305},
  title    = {Compositional semantic parsing on semi-structured tables},
  year     = {2015},
  abstract = {Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering
complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and
show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.},
  comment  = {Citations - 723 (13/101/2024)},
  url      = {https://arxiv.org/pdf/1508.00305},
}

@InProceedings{Jin2022,
  author       = {Jin, Nengzheng and Siebert, Joanna and Li, Dongfang and Chen, Qingcai},
  booktitle    = {China Conference on Knowledge Graph and Semantic Computing},
  title        = {A survey on table question answering: recent advances},
  year         = {2022},
  organization = {Springer},
  pages        = {174--186},
  abstract     = {Table Question Answering (Table QA) refers to providing precise answers from tables to answer a user‚Äôs question. In recent years, there have been a lot of works on table QA, but there is a lack of comprehensive surveys on this research topic. Hence, we aim to provide an overview of available datasets and representative methods in table QA. We classify existing methods for table QA into five categories according to their techniques, which include semantic-parsing-based, generative,
extractive, matching-based, and retriever-reader-based methods. Moreover, because table QA is still a challenging task for existing methods, we also identify and outline several key challenges and discuss the potential future directions of table QA.},
  comment      = {Citations -37 (13/10/2024)},
  url          = {https://arxiv.org/pdf/2207.05270},
}

@Article{Zhong2017,
  author   = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  journal  = {arXiv preprint arXiv:1709.00103},
  title    = {Seq2sql: Generating structured queries from natural language using reinforcement learning},
  year     = {2017},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.},
  comment  = {Citations - 1155 (13/10/2024)},
  url      = {https://arxiv.org/pdf/1709.00103},
}

@Article{Xu2017,
  author   = {Xu, Xiaojun and Liu, Chang and Song, Dawn},
  journal  = {arXiv preprint arXiv:1711.04436},
  title    = {Sqlnet: Generating structured queries from natural language without reinforcement learning},
  year     = {2017},
  abstract = {Synthesizing SQL queries from natural language is a long-standing open problem and has been attracting considerable interest recently. Toward solving the problem, the de facto approach is to employ a sequence-to-sequence-style model. Such an approach will necessarily require the SQL queries to be serialized. Since the same SQL query may have multiple equivalent serializations, training a sequence-to-sequence-style model is sensitive to the choice from one of them. This phenomenon is documented as the "order-matters" problem. Existing state-of-the-art approaches rely on reinforcement learning to reward the decoder when it generates any of the equivalent serializations. However, we observe that the improvement from reinforcement learning is limited. In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally solve this problem by avoiding the sequence-to-sequence structure when the order does not matter. In particular, we employ a sketch-based approach where the sketch contains a dependency graph so that one prediction can be done by taking into consideration only the previous predictions that it depends on. In addition, we propose a sequence-to-set model as well as the column attention mechanism to synthesize the query based on the sketch. By combining all these novel techniques, we show that SQLNet can outperform the prior art by 9% to 13% on the WikiSQL task.},
  comment  = {Citations - 431 (13/10/2024)},
  url      = {https://arxiv.org/pdf/1711.04436},
}

@Article{Yu2018,
  author   = {Yu, Tao and Li, Zifan and Zhang, Zilin and Zhang, Rui and Radev, Dragomir},
  journal  = {arXiv preprint arXiv:1804.09769},
  title    = {Typesql: Knowledge-based type-aware neural text-to-sql generation},
  year     = {2018},
  abstract = {Interacting with relational databases through natural language helps users of any background easily query and analyze a vast amount of data. This requires a system that understands users' questions and converts them to SQL queries automatically. In this paper we present a novel approach, TypeSQL, which views this problem as a slot filling task. Additionally, TypeSQL utilizes type information to better understand rare entities and numbers in natural language questions. We test this idea on the WikiSQL dataset and outperform the prior state-of-the-art by 5.5% in much less time. We also show that accessing the content of databases can significantly improve the performance when users' queries are not well-formed. TypeSQL gets 82.6% accuracy, a 17.5% absolute improvement compared to the previous content-sensitive model.},
  comment  = {Citations - 295 (13/10/2024)},
  url      = {https://arxiv.org/pdf/1804.09769},
}

@Article{Abraham2022,
  author   = {Abraham, Abhijith Neil and Rahman, Fariz and Kaur, Damanpreet},
  journal  = {arXiv preprint arXiv:2202.00454},
  title    = {Tablequery: Querying tabular data with natural language},
  year     = {2022},
  abstract = {This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery.},
  comment  = {Citations - 2 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2202.00454},
}

@Article{Wei2022,
  author   = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal  = {Advances in neural information processing systems},
  title    = {Chain-of-thought prompting elicits reasoning in large language models},
  year     = {2022},
  pages    = {24824--24837},
  volume   = {35},
  abstract = {We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  comment  = {Citations - 7741 (14/10/2024)},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
}

@Article{Zhou2023,
  author   = {Zhou, Houquan and Hou, Yang and Li, Zhenghua and Wang, Xuebin and Wang, Zhefeng and Duan, Xinyu and Zhang, Min},
  journal  = {arXiv preprint arXiv:2311.08287},
  title    = {How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions},
  year     = {2023},
  abstract = {While recent advancements in large language models (LLMs) bring us closer to achieving artificial general intelligence, the question persists: Do LLMs truly understand language, or do they merely mimic comprehension through pattern recognition? This study seeks to explore this question through the lens of syntax, a crucial component of sentence comprehension. Adopting a natural language question-answering (Q&A) scheme, we craft questions targeting nine syntactic knowledge points that are most closely related to sentence comprehension. Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points. In particular, questions involving prepositional phrase attachment pose the greatest challenge, whereas those concerning adjectival modifier and indirect object are relatively easier for LLMs to handle. Furthermore, a case study on the training dynamics of the LLMs reveals that the majority of syntactic knowledge is learned during the initial stages of training, hinting that simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLMs.},
  comment  = {Citations - 1(14/10/2024)},
  url      = {https://arxiv.org/pdf/2311.08287},
}

@Article{Jaitly2023,
  author   = {Jaitly, Sukriti and Shah, Tanay and Shugani, Ashish and Grewal, Razik Singh},
  journal  = {arXiv preprint arXiv:2312.12464},
  title    = {Towards Better Serialization of Tabular Data for Few-shot Classification},
  year     = {2023},
  abstract = {We present a study on the integration of Large Language Models (LLMs) in tabular data classification, emphasizing an efficient framework. Building upon existing work done in TabLLM (arXiv:2210.10723), we introduce three novel serialization techniques, including the standout LaTeX serialization method. This method significantly boosts the performance of LLMs in processing domain-specific datasets, Our method stands out for its memory efficiency and ability to fully utilize complex data structures. Through extensive experimentation, including various serialization approaches like feature combination and importance, we demonstrate our work's superiority in accuracy and efficiency over traditional models.},
  comment  = {Citations - 2 (14/10/2024)},
  url      = {https://arxiv.org/pdf/2312.12464},
}

@Article{Wang2024,
  author   = {Wang, Zilong and Zhang, Hao and Li, Chun-Liang and Eisenschlos, Julian Martin and Perot, Vincent and Wang, Zifeng and Miculicich, Lesly and Fujii, Yasuhisa and Shang, Jingbo and Lee, Chen-Yu and others},
  journal  = {arXiv preprint arXiv:2401.04398},
  title    = {Chain-of-table: Evolving tables in the reasoning chain for table understanding},
  year     = {2024},
  abstract = {Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the CHAIN-OF-TABLE framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the
intermediate results, enabling more accurate and reliable predictions. CHAINOF-TABLE achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.},
  comment  = {Citations - 34 (14/10/2024)},
  url      = {https://arxiv.org/pdf/2401.04398},
}

@Article{Cheng2022,
  author   = {Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and others},
  journal  = {arXiv preprint arXiv:2210.02875},
  title    = {Binding language models in symbolic languages},
  year     = {2022},
  abstract = {Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .},
  comment  = {Citations - 139 (15/10/2024)},
  url      = {https://arxiv.org/pdf/2210.02875},
}

@Article{Liu2023,
  author   = {Liu, Tianyang and Wang, Fei and Chen, Muhao},
  journal  = {arXiv preprint arXiv:2312.16702},
  title    = {Rethinking Tabular Data Understanding with Large Language Models},
  year     = {2023},
  abstract = {Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WIKITABLEQUESTIONS, representing a substantial advancement over previous existing table processing paradigms of LLMs.},
  comment  = {Citations - 9 (15/10/2024)},
  url      = {https://arxiv.org/pdf/2312.16702},
}

@Article{Ruan2024,
  author   = {Ruan, Yucheng and Lan, Xiang and Ma, Jingying and Dong, Yizhi and He, Kai and Feng, Mengling},
  journal  = {arXiv preprint arXiv:2408.10548},
  title    = {Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution},
  year     = {2024},
  abstract = {Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.},
  comment  = {Citations - 0 (15/10/2024)},
  url      = {https://arxiv.org/pdf/2408.10548},
}

@InProceedings{Sui2024,
  author    = {Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
  booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
  title     = {Table meets llm: Can large language models understand structured table data? a benchmark and empirical study},
  year      = {2024},
  pages     = {645--654},
  abstract  = {Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. Although tables can be used as input to LLMs with serialization, there is a lack of comprehensive studies that examine whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We perform a series of evaluations on GPT-3.5 and GPT-4. We find that performance varied depending on several input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we proposeself-augmentation for effective structural prompting, such as critical value / range identification using internal knowledge of LLMs. When combined with carefully chosen input choices, these structural prompting methods lead to promising improvements in LLM performance on a variety of tabular tasks, \eg, TabFact(\uparrow2.31%), HybridQA(\uparrow2.13%), SQA(\uparrow2.72%), Feverous(\uparrow0.84%), and ToTTo(\uparrow5.68%). We believe that our open-source (please find code and data at https://github.com/microsoft/TableProvider) benchmark and proposed prompting methods can serve as a simple yet generic selection for future research.},
  comment   = {Citations - 46 (16/10/2024)},
  url       = {https://dl.acm.org/doi/pdf/10.1145/3616855.3635752},
}

@Article{Singha2023,
  author   = {Singha, Ananya and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Parnin, Chris},
  journal  = {arXiv preprint arXiv:2310.10358},
  title    = {Tabular representation, noisy operators, and impacts on table structure understanding tasks in llms},
  year     = {2023},
  abstract = {Large language models (LLMs) are increasingly applied for tabular tasks using in-context learning. The prompt representation for a table may play a role in the LLMs ability to process the table. Inspired by prior work, we generate a collection of self-supervised structural tasks (e.g. navigate to a cell and row; transpose the table) and evaluate the performance differences when using 8 formats. In contrast to past work, we introduce 8 noise operations inspired by real-world messy data and adversarial inputs, and show that such operations can impact LLM performance across formats for different structural understanding tasks.},
  comment  = {Citations - 19 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2310.10358},
}

@Article{Zhang2023,
  author   = {Zhang, Hengyuan and Chang, Peng and Ji, Zongcheng},
  journal  = {arXiv preprint arXiv:2308.11891},
  title    = {Bridging the Gap: Deciphering Tabular Data Using Large Language Model},
  year     = {2023},
  abstract = {In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we've instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about 1.2% in tests on specific datasets. This research marks the first application of large language models to table-based question answering tasks, enhancing the model's comprehension of both table structures and content.},
  comment  = {Citations - 0 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2308.11891},
}

@InProceedings{Ye2023,
  author    = {Ye, Yunhu and Hui, Binyuan and Yang, Min and Li, Binhua and Huang, Fei and Li, Yongbin},
  booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  title     = {Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning},
  year      = {2023},
  pages     = {174--184},
  abstract  = {Table-based reasoning has shown remarkable progress in a wide range of tablebased tasks. It is a challenging task, which requires reasoning over both free-form natural language (NL) questions and (semi-)structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on ‚Äúhuge‚Äù evidence (tables). In addition, most existing methods struggle to reason over complex questions since the essential information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning, and (ii) decompose a complex question into simpler sub-questions for text reasoning. First, we use a
powerful LLM to decompose the evidence involved in the current question into the sub-evidence that retains the relevant information and excludes the remaining irrelevant information from the ‚Äúhuge‚Äù evidence. Second, we propose a novel ‚Äúparsing-execution-filling‚Äù strategy to decompose a complex question into simper step-by-step sub-questions by generating intermediate SQL queries as a bridge
to produce numerical and logical sub-questions with a powerful LLM. Finally, we leverage the decomposed sub-evidence and sub-questions to get the final answer with a few in-context prompting examples. Extensive experiments on three benchmark datasets (TabFact, WikiTableQuestion, and FetaQA) demonstrate that our method achieves significantly better results than competitive baselines for table-based reasoning. Notably, our method outperforms human performance for the first time on the TabFact dataset. In addition to impressive overall performance, our method also has the advantage of interpretability, where the returned results are to some extent tractable with the generated sub-evidence and sub-questions.},
  comment   = {Citations - 45 (16/10/2024)},
  url       = {https://arxiv.org/pdf/2301.13808},
}

@Article{Narayan2022,
  author   = {Narayan, Avanika and Chami, Ines and Orr, Laurel and Arora, Simran and R{\'e}, Christopher},
  journal  = {arXiv preprint arXiv:2205.09911},
  title    = {Can foundation models wrangle your data?},
  year     = {2022},
  abstract = {Foundation Models (FMs) are models trained on large corpora of data that, at very large scale, can generalize to new tasks without any task-specific finetuning. As these models continue to grow in size, innovations continue to push the boundaries of what these models can do on language and image tasks. This paper aims to understand an underexplored area of FMs: classical data tasks like cleaning and integration. As a proof-of-concept, we cast five data cleaning and integration tasks as prompting tasks and evaluate the performance of FMs on these tasks. We find that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks. We identify specific research challenges and opportunities that these models present, including challenges with private and domain specific data, and opportunities to make data management systems more accessible to non-experts. We make our code and experiments publicly available at: https://github.com/HazyResearch/fm_data_tasks.},
  comment  = {Citations - 154 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2205.09911},
}

@Article{Wang2023,
  author   = {Wang, Zifeng and Gao, Chufan and Xiao, Cao and Sun, Jimeng},
  journal  = {arXiv preprint arXiv:2305.12081},
  title    = {MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement},
  year     = {2023},
  abstract = {Tabular data prediction has been employed in medical applications such as patient health risk rediction. However, existing methods usually revolve around the algorithm design while overlooking the
significance of data engineering. Medical tabular datasets frequently exhibit significant heterogeneity across different sources, with limited sample sizes per source. As such, previous predictors are often trained on manually curated small datasets that struggle to generalize across different tabular datasets during inference. This paper proposes to scale medical tabular data predictors (MediTab)
to various tabular inputs with varying features. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with distinct schema. It also aligns out-domain data with the target task using a ‚Äúlearn, annotate, and refinement‚Äù pipeline. The expanded training data then enables the pre-trained MediTab to infer for arbitrary tabular input in the domain without finetuning, resulting in significant improvements over
supervised baselines: it reaches an average ranking of 1.57 and 1.00 on 7 patient outcome prediction
datasets and 3 trial outcome prediction datasets, respectively. In addition, MediTab exhibits impressive zero-shot performances: it outperforms supervised XGBoost models by 8.9% and 17.2% on average in two prediction tasks, respectively.},
  comment  = {Citations - 4 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2305.12081},
}

@Article{Yu2023,
  author   = {Yu, Bowen and Fu, Cheng and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  journal  = {arXiv preprint arXiv:2306.16762},
  title    = {Unified language representation for question answering over text, tables, and images},
  year     = {2023},
  abstract = {When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task into a simpler textual QA problem that can be solved using three steps: retrieval, ranking, and generation, all within a language space. This idea takes advantage of the power of pre-trained language models and is implemented in a framework called Solar. Our experimental results show that Solar outperforms all existing methods by 10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different metrics. Additionally, Solar achieves the best performance on the WebQA leaderboard},
  comment  = {Citations - 13 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2306.16762},
}

@InProceedings{Hegselmann2023,
  author       = {Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  title        = {Tabllm: Few-shot classification of tabular data with large language models},
  year         = {2023},
  organization = {PMLR},
  pages        = {5549--5581},
  abstract     = {We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method‚Äôs ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.},
  comment      = {Citations - 186 (16/10/2024)},
  url          = {https://proceedings.mlr.press/v206/hegselmann23a/hegselmann23a.pdf},
}

@InProceedings{Gong2020,
  author    = {Gong, Heng and Sun, Yawei and Feng, Xiaocheng and Qin, Bing and Bi, Wei and Liu, Xiaojiang and Liu, Ting},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  title     = {Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching},
  year      = {2020},
  pages     = {1978--1988},
  abstract  = {Although neural table-to-text models have achieved remarkable progress with the help of large-scale datasets, they suffer insufficient learning problem with limited training data. Recently, pre-trained language models show potential in few-shot learning with linguistic knowledge learnt from pretraining on large-scale corpus. However, benefiting table-to-text generation in few-shot setting with the powerful pretrained language model faces three challenges, including (1) the gap between the task‚Äôs structured input and the natural language input for pretraining language model.(2) The lack of modeling for table structure and (3) improving text fidelity with less incorrect expressions that are contradicting to the table. To address aforementioned problems, we propose TableGPT for table-to-text generation. At first, we utilize table transformation module with template to rewrite structured table in natural language as input for GPT-2. In addition, we exploit multi-task learning with two auxiliary tasks that preserve table‚Äôs structural information by reconstructing the structure from GPT-2‚Äôs representation and improving the text‚Äôs fidelity with content matching task aligning the table and information in the generated text. By experimenting on Humans, Songs and Books, three few-shot table-to-text datasets in different domains, our model outperforms existing systems on most few-shot settings.},
  comment   = {Citations - 82 (16/10/2024)},
  url       = {https://aclanthology.org/2020.coling-main.179.pdf},
}

@Article{Dinh2022,
  author   = {Dinh, Tuan and Zeng, Yuchen and Zhang, Ruisu and Lin, Ziqian and Gira, Michael and Rajput, Shashank and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
  journal  = {Advances in Neural Information Processing Systems},
  title    = {Lift: Language-interfaced fine-tuning for non-language machine learning tasks},
  year     = {2022},
  pages    = {11763--11784},
  volume   = {35},
  abstract = {Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling" no-code machine learning with LMs." We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, eg, context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github. com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.},
  comment  = {Citations - 99 (16/10/2024)},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/file/4ce7fe1d2730f53cb3857032952cd1b8-Paper-Conference.pdf},
}

@Article{Chen2023,
  author   = {Chen, Nuo and Shou, Linjun and Gong, Ming and Pei, Jian and You, Chenyu and Chang, Jianhui and Jiang, Daxin and Li, Jia},
  journal  = {arXiv preprint arXiv:2302.09302},
  title    = {Bridge the gap between language models and tabular understanding},
  year     = {2023},
  abstract = {Table pretrain-then-finetune paradigm has been proposed and employed at a rapid pace after the success of pre-training in the natural language domain. Despite the promising findings in tabular pre-trained language models (TPLMs), there is an input gap between pre-training and fine-tuning phases. For instance, TPLMs jointly pre-trained with table and text input could be effective for tasks also with table-text joint input like table question answering, but it may fail for tasks with only tables or text as input such as table retrieval. To this end, we propose UTP, an approach that dynamically supports three types of multi-modal inputs: table-text, table, and text. Specifically, UTP is pre-trained with two strategies: (1) We first utilize a universal mask language modeling objective on each kind of input, enforcing the model to adapt various inputs. (2) We then present Cross-Modal Contrastive Regularization (CMCR), which utilizes contrastive learning to encourage the consistency between table-text cross-modality representations via unsupervised instance-wise training signals during pre-training. By these means, the resulting model not only bridges the input gap between pre-training and fine-tuning but also advances in the alignment of table and text. Extensive results show UTP achieves superior results on uni-modal input tasks (e.g., table retrieval) and cross-modal input tasks (e.g., table question answering).},
  comment  = {Citations - 4 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2302.09302},
}

@Article{Cong2023,
  author   = {Cong, Tianji and Hulsebos, Madelon and Sun, Zhenjie and Groth, Paul and Jagadish, HV},
  journal  = {arXiv preprint arXiv:2310.07736},
  title    = {Observatory: Characterizing embeddings of relational tables},
  year     = {2023},
  abstract = {Language models and specialized table embedding models have recently demonstrated strong performance on many tasks over tabular data. Researchers and practitioners are keen to leverage these models in many new application contexts; but limited understanding of the strengths and weaknesses of these models, and the table representations they generate, makes the process of finding a suitable model for a given task reliant on trial and error. There is an urgent need to gain a comprehensive understanding of these models to minimize inefficiency and failures in downstream usage. To address this need, we propose Observatory, a formal framework to systematically analyze embedding representations of relational tables. Motivated both by invariants of the relational data model and by statistical considerations regarding data distributions, we define eight primitive properties, and corresponding measures to quantitatively characterize table embeddings for these properties. Based on these properties, we define an extensible framework to evaluate language and table embedding models. We collect and synthesize a suite of datasets and use Observatory to analyze seven such models. Our analysis provides insights into the strengths and weaknesses of learned representations over tables. We find, for example, that some models are sensitive to table structure such as column order, that functional dependencies are rarely reflected in embeddings, and that specialized table embedding models have relatively lower sample fidelity. Such insights help researchers and practitioners better anticipate model behaviors and select appropriate models for their downstream tasks, while guiding researchers in the development of new models.},
  comment  = {Citations - 8 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2310.07736},
}

@Article{Sarkar2023,
  author   = {Sarkar, Soumajyoti and Lausen, Leonard},
  journal  = {arXiv preprint arXiv:2310.00789},
  title    = {Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks},
  year     = {2023},
  abstract = {Tables stored in databases and tables which are present in web pages and articles account for a large part of semi-structured data that is available on the internet. It then becomes pertinent to develop a modeling approach with large language models (LLMs) that can be used to solve diverse table tasks such as semantic parsing, question answering as well as classification problems. Traditionally, there existed separate models specialized for each task individually. It raises the question of how far can we go to build a unified model that works well on some table tasks without significant degradation on others. To that end, we attempt at creating a shared modeling approach in the pretraining stage with encoder-decoder style LLMs that can cater to diverse tasks. We evaluate our approach that continually pretrains and finetunes different model families of T5 with data from tables and surrounding context, on these downstream tasks at different model scales. Through multiple ablation studies, we observe that our pretraining with self-supervised objectives can significantly boost the performance of the models on these tasks. As an example of one improvement, we observe that the instruction finetuned public models which come specialized on text question answering (QA) and have been trained on table data still have room for improvement when it comes to table specific QA. Our work is the first attempt at studying the advantages of a unified approach to table specific pretraining when scaled from 770M to 11B sequence to sequence models while also comparing the instruction finetuned variants of the models.},
  comment  = {Citatiosn - 2 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2402.17944},
}

@Article{Zha2023,
  author   = {Zha, Liangyu and Zhou, Junlin and Li, Liyao and Wang, Rui and Huang, Qingyi and Yang, Saisai and Yuan, Jing and Su, Changbao and Li, Xiang and Su, Aofeng and others},
  journal  = {arXiv preprint arXiv:2307.08674},
  title    = {Tablegpt: Towards unifying tables, nature language and commands into one gpt},
  year     = {2023},
  abstract = {Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.},
  comment  = {Citations - 15 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2307.08674},
}

@Article{Zhang2023a,
  author   = {Zhang, Tianshu and Yue, Xiang and Li, Yifei and Sun, Huan},
  journal  = {arXiv preprint arXiv:2311.09206},
  title    = {Tablellama: Towards open large generalist models for tables},
  year     = {2023},
  abstract = {Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model's generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables.},
  comment  = {Citations - 40 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2311.09206},
}

@Article{Li2023a,
  author   = {Li, Peng and He, Yeye and Yashar, Dror and Cui, Weiwei and Ge, Song and Zhang, Haidong and Fainman, Danielle Rifinski and Zhang, Dongmei and Chaudhuri, Surajit},
  journal  = {arXiv preprint arXiv:2310.09263},
  title    = {Table-gpt: Table-tuned gpt for diverse table tasks},
  year     = {2023},
  abstract = {Language models, such as GPT-3 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models
using a range of basic table-understanding tasks, we observe that today‚Äôs language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on onedimensional natural-language texts, whereas relational tables are two-dimensional objects.
In this work, we propose a new ‚Äútable-tuning‚Äù paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as
training data, with the goal of enhancing language models‚Äô ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better table-understanding
capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout unseen tasks, and (2) strong generalizability, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.},
  comment  = {Citations - 37 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2310.09263},
}

@Article{Liu2021,
  author   = {Liu, Qian and Chen, Bei and Guo, Jiaqi and Ziyadi, Morteza and Lin, Zeqi and Chen, Weizhu and Lou, Jian-Guang},
  journal  = {arXiv preprint arXiv:2107.07653},
  title    = {TAPEX: Table pre-training via learning a neural SQL executor},
  year     = {2021},
  abstract = {Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we propose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL executor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes the improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs and to achieve new state-of-the-art results on various downstream tasks. Our code can be found at https://github.com/microsoft/Table-Pretraining.},
  comment  = {Citations - 198 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2107.07653},
}

@Article{Chang2023,
  author   = {Chang, Shuaichen and Fosler-Lussier, Eric},
  journal  = {arXiv preprint arXiv:2305.11853},
  title    = {How to prompt llms for text-to-sql: A study in zero-shot, single-domain, and cross-domain settings},
  year     = {2023},
  abstract = {Large language models (LLMs) with in-context learning have demonstrated remarkable capability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and intermediate reasoning steps to enhance the performance of LLMs. However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across various settings and provide insights into prompt constructions for future text-to-SQL studies.},
  comment  = {Citations - 35 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2305.11853},
}

@Article{Zhao2023,
  author   = {Zhao, Yilun and Zhao, Chen and Nan, Linyong and Qi, Zhenting and Zhang, Wenlin and Tang, Xiangru and Mi, Boyu and Radev, Dragomir},
  journal  = {arXiv preprint arXiv:2306.14321},
  title    = {Robut: A systematic study of table qa robustness against human-annotated adversarial perturbations},
  year     = {2023},
  abstract = {Despite significant progress having been made in question answering on tabular data (Table QA), it's unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the robustness of Table QA models, we propose a benchmark called RobuT, which builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and includes human-annotated adversarial perturbations in terms of table header, table content, and question. Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets. We propose to address this problem by using large language models to generate adversarial examples to enhance training, which significantly improves the robustness of Table QA models. Our data and code is publicly available at https://github.com/yilunzhao/RobuT.},
  comment  = {Citations - 14 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2306.14321},
}

@Article{Chen2022,
  author   = {Chen, Wenhu},
  journal  = {arXiv preprint arXiv:2210.06710},
  title    = {Large language models are few (1)-shot table reasoners},
  year     = {2022},
  abstract = {Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very strong performance with only a 1-shot demonstration, even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in https://github.com/wenhuchen/TableCoT.},
  comment  = {Citations - 100 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2210.06710},
}

@Article{Sundar2023,
  author   = {Sundar, Anirudh S and Heck, Larry},
  journal  = {arXiv preprint arXiv:2303.12024},
  title    = {cTBLS: Augmenting large language models with conversational tables},
  year     = {2023},
  abstract = {Optimizing accuracy and performance while eliminating hallucinations of open-domain conversational large language models (LLMs) is an open research challenge. A particularly promising direction is to augment and ground LLMs with information from structured sources. This paper introduces Conversational Tables (cTBLS), a three-step architecture to retrieve and generate dialogue responses grounded on retrieved tabular information. cTBLS uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 125% relative improvement over the retriever in the previous state-of-the-art system on the HyrbiDialogue dataset. cTBLS then uses a shared process between encoder and decoder models to perform a coarse+fine tabular knowledge (e.g., cell) ranking combined with a GPT-3.5 LLM response generator to yield a 2x relative improvement in ROUGE scores. Finally, human evaluators prefer cTBLs +80% of the time (coherency, fluency) and judge informativeness to be 4x better than the previous state-of-the-art.},
  comment  = {Citations - 10 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2303.12024},
}

@InProceedings{Liu2023a,
  author    = {Liu, Shang-Ching and Wang, ShengKun and Chang, Tsungyao and Lin, Wenqi and Hsiung, Chung-Wei and Hsieh, Yi-Chen and Cheng, Yu-Ping and Luo, Sian-Hong and Zhang, Jianwei},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  title     = {JarviX: A LLM no code platform for tabular data analysis and optimization},
  year      = {2023},
  pages     = {622--630},
  abstract  = {In this study, we introduce JarviX, a sophisticated data analytics framework. JarviX is designed to employ Large Language Models (LLMs) to facilitate an automated guide and execute high-precision data analyzes on tabular datasets. This framework emphasizes the significance of varying column types, capitalizing on state-of-the-art LLMs to generate concise data insight summaries, propose relevant analysis inquiries, visualize data effectively, and provide comprehensive explanations for results drawn from an extensive data analysis pipeline. Moreover, JarviX incorporates an automated machine learning (AutoML) pipeline for predictive modeling. This integration forms a comprehensive and automated optimization cycle, which proves particularly advantageous for optimizing machine configuration. The efficacy and adaptability of JarviX are substantiated through a series of practical use case studies.},
  comment   = {Citations - 6 (16/10/2024)},
  url       = {https://aclanthology.org/2023.emnlp-industry.59.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}
