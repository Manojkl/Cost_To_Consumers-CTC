@Article{Fang2024,
  author   = {Fang, Xi and Xu, Weijie and Tan, Fiona Anting and Zhang, Jiani and Hu, Ziqing and Qi, Yanjun Jane and Nickleach, Scott and Socolinsky, Diego and Sengamedu, Srinivasan and Faloutsos, Christos and others},
  title    = {Large language models (LLMs) on tabular data: Prediction, generation, and understanding-a survey},
  year     = {2024},
  abstract = {Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.},
  comment  = {Citation -27},
  ranking  = {rank5},
  url      = {https://www.amazon.science/publications/large-language-models-llms-on-tabular-data-prediction-generation-and-understanding-a-survey},
}

@Misc{HongWei2024,
  author   = {Hong-Wei, Wu},
  note     = {Accessed: 2024-05-30},
  title    = {Awesome-LLM-Tabular},
  year     = {2024},
  abstract = {Since the emergence of ChatGPT, Large Language Models (LLMs) have garnered significant attention, with new advancements continuously emerging. LLMs have found applications in various domains like vision, audio, and text tasks. However, tabular data remains a crucial data format in this world. Hence, this repo focuses on collecting research papers that explore the integration of LLM technology with tabular data, and aims to save you valuable time and boost research efficiency.

Awesome-LLM-Tabular is a curated list of Large Language Model applied to Tabular Data.

This project is currently under development. Feel free to ‚≠ê (STAR) and üî≠ (WATCH) it to stay updated on the latest developments.},
  orcid    = {https://orcid.org/0009-0005-8073-5297},
  url      = {https://github.com/johnnyhwu/Awesome-LLM-Tabular},
}

@Misc{Lu,
  author       = {Weizheng Lu},
  howpublished = {\url{https://github.com/godaai/llm-table-survey}},
  note         = {[Accessed 12-10-2024]},
  title        = {GitHub - llm-table-survey},
  abstract     = {Survey on Tabular LLM},
  url          = {https://github.com/godaai/llm-table-survey},
}

@InProceedings{Li2023,
  author    = {Li, Hongxin and Su, Jingran and Chen, Yuntao and Li, Qing and ZHANG, ZHAO-XIANG},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {{SheetCopilot}: {Bringing} {Software} {Productivity} to the {Next} {Level} through {Large} {Language} {Models}},
  year      = {2023},
  editor    = {Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  pages     = {4952--4984},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0ff30c4bf31db0119a6219e0d250e037-Paper-Conference.pdf},
}

@Article{Zhang2024,
  author    = {Zhang, Weixu and Wang, Yifei and Song, Yuanfeng and Wei, Victor Junqiu and Tian, Yuxing and Qi, Yiyan and Chan, Jonathan H and Wong, Raymond Chi-Wing and Yang, Haiqin},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  title     = {Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey},
  year      = {2024},
  abstract  = {The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.},
  comment   = {Citations - 6 (12/10/2024)},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/2310.17894},
}

@InProceedings{Zhao2024,
  author    = {Zhao, Yilun and Long, Yitao and Liu, Hongjun and Kamoi, Ryo and Nan, Linyong and Chen, Lyuhao and Liu, Yixin and Tang, Xiangru and Zhang, Rui and Cohan, Arman},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Financial Documents},
  year      = {2024},
  pages     = {16103--16120},
  abstract  = {Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing financial documents containing both text and tables. We evaluate a wide spectrum of 27 LLMs, including those specialized in math, coding and finance, with Chain-of-Thought and Program-of-Thought prompting methods. We found that even the current best-performing system (ie, GPT-4) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe DocMath-Eval can be used as a valuable benchmark to evaluate LLMs‚Äô capabilities to solve challenging numerical reasoning problems in expert domains.},
  comment   = {Citations - 1 (12/10/2024)},
  url       = {https://aclanthology.org/2024.acl-long.852.pdf},
}

@Article{Sui2023,
  author   = {Sui, Yuan and Zou, Jiaru and Zhou, Mengyu and He, Xinyi and Du, Lun and Han, Shi and Zhang, Dongmei},
  journal  = {arXiv preprint arXiv:2312.09039},
  title    = {Tap4llm: Table provider on sampling, augmenting, and packing semi-structured data for large language model reasoning},
  year     = {2023},
  abstract = {Table reasoning tasks have shown remarkable progress with the development of large language models (LLMs), which involve interpreting and drawing conclusions from tabular data based on natural language (NL) questions. Existing solutions mainly tested on smaller tables face scalability issues and struggle with complex queries due to incomplete or dispersed data across different table sections. To alleviate
these challenges, we propose TAP4LLM as a versatile pre-processor suite for leveraging LLMs in table-based tasks effectively. It covers several distinct components: (1) table sampling to decompose large tables into manageable subtables based on query semantics, (2) table augmentation to enhance tables with additional knowledge from external sources or models, and (3) table packing & serialization to convert tables into various formats suitable for LLMs‚Äô understanding. In each module, we design and
compare several common methods under various usage scenarios, aiming to shed light on the
best practices for leveraging LLMs for tablereasoning tasks. Our experiments show that
our method improves LLMs‚Äô reasoning capabilities in various tabular tasks and enhances
the interaction between LLMs and tabular data
by employing effective pre-processing.},
  comment  = {Citations - 10 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2312.09039},
}

@InProceedings{Wang2021,
  author    = {Wang, Zhiruo and Dong, Haoyu and Jia, Ran and Li, Jia and Fu, Zhiyi and Han, Shi and Zhang, Dongmei},
  booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  title     = {TUTA: Tree-based Transformers for Generally Structured Table Pre-training},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {1780‚Äì1790},
  publisher = {Association for Computing Machinery},
  series    = {KDD '21},
  abstract  = {We propose TUTA, a unified pre-training architecture for understanding generally structured tables. Noticing that understanding a table requires spatial, hierarchical, and semantic information, we enhance transformers with three novel structure-aware mechanisms. First, we devise a unified tree-based structure, called a bi-dimensional coordinate tree, to describe both the spatial and hierarchical information of generally structured tables. Upon this, we propose tree-based attention and position embedding to better capture the spatial and hierarchical information. Moreover, we devise three progressive pre-training objectives to enable representations at the token, cell, and table levels. We pre-train TUTA on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two critical tasks in the field of table structure understanding: cell type classification and table type classification. Experiments show that TUTA is highly effective, achieving state-of-the-art on five widely-studied datasets.},
  doi       = {10.1145/3447548.3467434},
  isbn      = {9781450383325},
  keywords  = {transformer, self supervision, generally structured table},
  location  = {Virtual Event, Singapore},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3447548.3467434},
}

@Article{Deng2022,
  author     = {Deng, Xiang and Sun, Huan and Lees, Alyssa and Wu, You and Yu, Cong},
  journal    = {SIGMOD Rec.},
  title      = {TURL: Table Understanding through Representation Learning},
  year       = {2022},
  issn       = {0163-5808},
  month      = jun,
  number     = {1},
  pages      = {33‚Äì40},
  volume     = {51},
  abstract   = {Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task-specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine-tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in a self-supervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning.},
  address    = {New York, NY, USA},
  comment    = {Citations - 29 (13/10/2024)},
  doi        = {10.1145/3542700.3542709},
  issue_date = {March 2022},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3542700.3542709},
}

@Article{Yin2020,
  author   = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
  journal  = {arXiv preprint arXiv:2005.08314},
  title    = {TaBERT: Pretraining for joint understanding of textual and tabular data},
  year     = {2020},
  abstract = {Recent years have witnessed the burgeoning of pretrained language models (LMs) for textbased natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both
free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TABERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TABERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TABERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WIKITABLEQUESTIONS,
while performing competitively on the text-toSQL dataset SPIDER.},
  comment  = {Citations -541 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2005.08314},
}

@InProceedings{Herzig2020,
  author    = {Herzig, Jonathan and Nowak, Pawel Krzysztof and M{\"u}ller, Thomas and Piccinno, Francesco and Eisenschlos, Julian},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  title     = {{T}a{P}as: Weakly Supervised Table Parsing via Pre-training},
  year      = {2020},
  address   = {Online},
  editor    = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  month     = jul,
  pages     = {4320--4333},
  publisher = {Association for Computational Linguistics},
  abstract  = {Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT{'}s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.},
  comment   = {Citations - 599(13/10/2024)},
  doi       = {10.18653/v1/2020.acl-main.398},
  url       = {https://aclanthology.org/2020.acl-main.398},
}

@InProceedings{Iida2021,
  author    = {Iida, Hiroshi and Thai, Dung and Manjunatha, Varun and Iyyer, Mohit},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {{TABBIE}: Pretrained Representations of Tabular Data},
  year      = {2021},
  address   = {Online},
  editor    = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  month     = jun,
  pages     = {3446--3456},
  publisher = {Association for Computational Linguistics},
  abstract  = {Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model{'}s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.},
  comment   = {Citations -},
  doi       = {10.18653/v1/2021.naacl-main.270},
  url       = {https://aclanthology.org/2021.naacl-main.270},
}

@Article{Pasupat2015,
  author   = {Pasupat, Panupong and Liang, Percy},
  journal  = {arXiv preprint arXiv:1508.00305},
  title    = {Compositional semantic parsing on semi-structured tables},
  year     = {2015},
  abstract = {Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering
complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and
show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.},
  comment  = {Citations - 723 (13/101/2024)},
  url      = {https://arxiv.org/pdf/1508.00305},
}

@InProceedings{Jin2022,
  author       = {Jin, Nengzheng and Siebert, Joanna and Li, Dongfang and Chen, Qingcai},
  booktitle    = {China Conference on Knowledge Graph and Semantic Computing},
  title        = {A survey on table question answering: recent advances},
  year         = {2022},
  organization = {Springer},
  pages        = {174--186},
  abstract     = {Table Question Answering (Table QA) refers to providing precise answers from tables to answer a user‚Äôs question. In recent years, there have been a lot of works on table QA, but there is a lack of comprehensive surveys on this research topic. Hence, we aim to provide an overview of available datasets and representative methods in table QA. We classify existing methods for table QA into five categories according to their techniques, which include semantic-parsing-based, generative,
extractive, matching-based, and retriever-reader-based methods. Moreover, because table QA is still a challenging task for existing methods, we also identify and outline several key challenges and discuss the potential future directions of table QA.},
  comment      = {Citations -37 (13/10/2024)},
  url          = {https://arxiv.org/pdf/2207.05270},
}

@Article{Zhong2017,
  author   = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  journal  = {arXiv preprint arXiv:1709.00103},
  title    = {Seq2sql: Generating structured queries from natural language using reinforcement learning},
  year     = {2017},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.},
  comment  = {Citations - 1155 (13/10/2024)},
  url      = {https://arxiv.org/pdf/1709.00103},
}

@Article{Xu2017,
  author   = {Xu, Xiaojun and Liu, Chang and Song, Dawn},
  journal  = {arXiv preprint arXiv:1711.04436},
  title    = {Sqlnet: Generating structured queries from natural language without reinforcement learning},
  year     = {2017},
  abstract = {Synthesizing SQL queries from natural language is a long-standing open problem and has been attracting considerable interest recently. Toward solving the problem, the de facto approach is to employ a sequence-to-sequence-style model. Such an approach will necessarily require the SQL queries to be serialized. Since the same SQL query may have multiple equivalent serializations, training a sequence-to-sequence-style model is sensitive to the choice from one of them. This phenomenon is documented as the "order-matters" problem. Existing state-of-the-art approaches rely on reinforcement learning to reward the decoder when it generates any of the equivalent serializations. However, we observe that the improvement from reinforcement learning is limited. In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally solve this problem by avoiding the sequence-to-sequence structure when the order does not matter. In particular, we employ a sketch-based approach where the sketch contains a dependency graph so that one prediction can be done by taking into consideration only the previous predictions that it depends on. In addition, we propose a sequence-to-set model as well as the column attention mechanism to synthesize the query based on the sketch. By combining all these novel techniques, we show that SQLNet can outperform the prior art by 9% to 13% on the WikiSQL task.},
  comment  = {Citations - 431 (13/10/2024)},
  url      = {https://arxiv.org/pdf/1711.04436},
}

@Article{Yu2018,
  author   = {Yu, Tao and Li, Zifan and Zhang, Zilin and Zhang, Rui and Radev, Dragomir},
  journal  = {arXiv preprint arXiv:1804.09769},
  title    = {Typesql: Knowledge-based type-aware neural text-to-sql generation},
  year     = {2018},
  abstract = {Interacting with relational databases through natural language helps users of any background easily query and analyze a vast amount of data. This requires a system that understands users' questions and converts them to SQL queries automatically. In this paper we present a novel approach, TypeSQL, which views this problem as a slot filling task. Additionally, TypeSQL utilizes type information to better understand rare entities and numbers in natural language questions. We test this idea on the WikiSQL dataset and outperform the prior state-of-the-art by 5.5% in much less time. We also show that accessing the content of databases can significantly improve the performance when users' queries are not well-formed. TypeSQL gets 82.6% accuracy, a 17.5% absolute improvement compared to the previous content-sensitive model.},
  comment  = {Citations - 295 (13/10/2024)},
  url      = {https://arxiv.org/pdf/1804.09769},
}

@Article{Abraham2022,
  author   = {Abraham, Abhijith Neil and Rahman, Fariz and Kaur, Damanpreet},
  journal  = {arXiv preprint arXiv:2202.00454},
  title    = {Tablequery: Querying tabular data with natural language},
  year     = {2022},
  abstract = {This paper presents TableQuery, a novel tool for querying tabular data using deep learning models pre-trained to answer questions on free text. Existing deep learning methods for question answering on tabular data have various limitations, such as having to feed the entire table as input into a neural network model, making them unsuitable for most real-world applications. Since real-world data might contain millions of rows, it may not entirely fit into the memory. Moreover, data could be stored in live databases, which are updated in real-time, and it is impractical to serialize an entire database to a neural network-friendly format each time it is updated. In TableQuery, we use deep learning models pre-trained for question answering on free text to convert natural language queries to structured queries, which can be run against a database or a spreadsheet. This method eliminates the need for fitting the entire data into memory as well as serializing databases. Furthermore, deep learning models pre-trained for question answering on free text are readily available on platforms such as HuggingFace Model Hub (7). TableQuery does not require re-training; when a newly trained model for question answering with better performance is available, it can replace the existing model in TableQuery.},
  comment  = {Citations - 2 (13/10/2024)},
  url      = {https://arxiv.org/pdf/2202.00454},
}

@Article{Wei2022,
  author   = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal  = {Advances in neural information processing systems},
  title    = {Chain-of-thought prompting elicits reasoning in large language models},
  year     = {2022},
  pages    = {24824--24837},
  volume   = {35},
  abstract = {We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  comment  = {Citations - 7741 (14/10/2024)},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
}

@Article{Zhou2023,
  author   = {Zhou, Houquan and Hou, Yang and Li, Zhenghua and Wang, Xuebin and Wang, Zhefeng and Duan, Xinyu and Zhang, Min},
  journal  = {arXiv preprint arXiv:2311.08287},
  title    = {How Well Do Large Language Models Understand Syntax? An Evaluation by Asking Natural Language Questions},
  year     = {2023},
  abstract = {While recent advancements in large language models (LLMs) bring us closer to achieving artificial general intelligence, the question persists: Do LLMs truly understand language, or do they merely mimic comprehension through pattern recognition? This study seeks to explore this question through the lens of syntax, a crucial component of sentence comprehension. Adopting a natural language question-answering (Q&A) scheme, we craft questions targeting nine syntactic knowledge points that are most closely related to sentence comprehension. Experiments conducted on 24 LLMs suggest that most have a limited grasp of syntactic knowledge, exhibiting notable discrepancies across different syntactic knowledge points. In particular, questions involving prepositional phrase attachment pose the greatest challenge, whereas those concerning adjectival modifier and indirect object are relatively easier for LLMs to handle. Furthermore, a case study on the training dynamics of the LLMs reveals that the majority of syntactic knowledge is learned during the initial stages of training, hinting that simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLMs.},
  comment  = {Citations - 1(14/10/2024)},
  url      = {https://arxiv.org/pdf/2311.08287},
}

@Article{Jaitly2023,
  author   = {Jaitly, Sukriti and Shah, Tanay and Shugani, Ashish and Grewal, Razik Singh},
  journal  = {arXiv preprint arXiv:2312.12464},
  title    = {Towards Better Serialization of Tabular Data for Few-shot Classification},
  year     = {2023},
  abstract = {We present a study on the integration of Large Language Models (LLMs) in tabular data classification, emphasizing an efficient framework. Building upon existing work done in TabLLM (arXiv:2210.10723), we introduce three novel serialization techniques, including the standout LaTeX serialization method. This method significantly boosts the performance of LLMs in processing domain-specific datasets, Our method stands out for its memory efficiency and ability to fully utilize complex data structures. Through extensive experimentation, including various serialization approaches like feature combination and importance, we demonstrate our work's superiority in accuracy and efficiency over traditional models.},
  comment  = {Citations - 2 (14/10/2024)},
  url      = {https://arxiv.org/pdf/2312.12464},
}

@Article{Wang2024,
  author   = {Wang, Zilong and Zhang, Hao and Li, Chun-Liang and Eisenschlos, Julian Martin and Perot, Vincent and Wang, Zifeng and Miculicich, Lesly and Fujii, Yasuhisa and Shang, Jingbo and Lee, Chen-Yu and others},
  journal  = {arXiv preprint arXiv:2401.04398},
  title    = {Chain-of-table: Evolving tables in the reasoning chain for table understanding},
  year     = {2024},
  abstract = {Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the CHAIN-OF-TABLE framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the
intermediate results, enabling more accurate and reliable predictions. CHAINOF-TABLE achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.},
  comment  = {Citations - 34 (14/10/2024)},
  url      = {https://arxiv.org/pdf/2401.04398},
}

@Article{Cheng2022,
  author   = {Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and others},
  journal  = {arXiv preprint arXiv:2210.02875},
  title    = {Binding language models in symbolic languages},
  year     = {2022},
  abstract = {Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .},
  comment  = {Citations - 139 (15/10/2024)},
  url      = {https://arxiv.org/pdf/2210.02875},
}

@Article{Liu2023,
  author   = {Liu, Tianyang and Wang, Fei and Chen, Muhao},
  journal  = {arXiv preprint arXiv:2312.16702},
  title    = {Rethinking Tabular Data Understanding with Large Language Models},
  year     = {2023},
  abstract = {Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WIKITABLEQUESTIONS, representing a substantial advancement over previous existing table processing paradigms of LLMs.},
  comment  = {Citations - 9 (15/10/2024)},
  url      = {https://arxiv.org/pdf/2312.16702},
}

@Article{Ruan2024,
  author   = {Ruan, Yucheng and Lan, Xiang and Ma, Jingying and Dong, Yizhi and He, Kai and Feng, Mengling},
  journal  = {arXiv preprint arXiv:2408.10548},
  title    = {Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution},
  year     = {2024},
  abstract = {Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.},
  comment  = {Citations - 0 (15/10/2024)},
  url      = {https://arxiv.org/pdf/2408.10548},
}

@InProceedings{Sui2024,
  author    = {Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
  booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
  title     = {Table meets llm: Can large language models understand structured table data? a benchmark and empirical study},
  year      = {2024},
  pages     = {645--654},
  abstract  = {Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. Although tables can be used as input to LLMs with serialization, there is a lack of comprehensive studies that examine whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We perform a series of evaluations on GPT-3.5 and GPT-4. We find that performance varied depending on several input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we proposeself-augmentation for effective structural prompting, such as critical value / range identification using internal knowledge of LLMs. When combined with carefully chosen input choices, these structural prompting methods lead to promising improvements in LLM performance on a variety of tabular tasks, \eg, TabFact(\uparrow2.31%), HybridQA(\uparrow2.13%), SQA(\uparrow2.72%), Feverous(\uparrow0.84%), and ToTTo(\uparrow5.68%). We believe that our open-source (please find code and data at https://github.com/microsoft/TableProvider) benchmark and proposed prompting methods can serve as a simple yet generic selection for future research.},
  comment   = {Citations - 46 (16/10/2024)},
  url       = {https://dl.acm.org/doi/pdf/10.1145/3616855.3635752},
}

@Article{Singha2023,
  author   = {Singha, Ananya and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Parnin, Chris},
  journal  = {arXiv preprint arXiv:2310.10358},
  title    = {Tabular representation, noisy operators, and impacts on table structure understanding tasks in llms},
  year     = {2023},
  abstract = {Large language models (LLMs) are increasingly applied for tabular tasks using in-context learning. The prompt representation for a table may play a role in the LLMs ability to process the table. Inspired by prior work, we generate a collection of self-supervised structural tasks (e.g. navigate to a cell and row; transpose the table) and evaluate the performance differences when using 8 formats. In contrast to past work, we introduce 8 noise operations inspired by real-world messy data and adversarial inputs, and show that such operations can impact LLM performance across formats for different structural understanding tasks.},
  comment  = {Citations - 19 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2310.10358},
}

@Article{Zhang2023,
  author   = {Zhang, Hengyuan and Chang, Peng and Ji, Zongcheng},
  journal  = {arXiv preprint arXiv:2308.11891},
  title    = {Bridging the Gap: Deciphering Tabular Data Using Large Language Model},
  year     = {2023},
  abstract = {In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we've instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about 1.2% in tests on specific datasets. This research marks the first application of large language models to table-based question answering tasks, enhancing the model's comprehension of both table structures and content.},
  comment  = {Citations - 0 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2308.11891},
}

@InProceedings{Ye2023,
  author    = {Ye, Yunhu and Hui, Binyuan and Yang, Min and Li, Binhua and Huang, Fei and Li, Yongbin},
  booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  title     = {Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning},
  year      = {2023},
  pages     = {174--184},
  abstract  = {Table-based reasoning has shown remarkable progress in a wide range of tablebased tasks. It is a challenging task, which requires reasoning over both free-form natural language (NL) questions and (semi-)structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on ‚Äúhuge‚Äù evidence (tables). In addition, most existing methods struggle to reason over complex questions since the essential information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning, and (ii) decompose a complex question into simpler sub-questions for text reasoning. First, we use a
powerful LLM to decompose the evidence involved in the current question into the sub-evidence that retains the relevant information and excludes the remaining irrelevant information from the ‚Äúhuge‚Äù evidence. Second, we propose a novel ‚Äúparsing-execution-filling‚Äù strategy to decompose a complex question into simper step-by-step sub-questions by generating intermediate SQL queries as a bridge
to produce numerical and logical sub-questions with a powerful LLM. Finally, we leverage the decomposed sub-evidence and sub-questions to get the final answer with a few in-context prompting examples. Extensive experiments on three benchmark datasets (TabFact, WikiTableQuestion, and FetaQA) demonstrate that our method achieves significantly better results than competitive baselines for table-based reasoning. Notably, our method outperforms human performance for the first time on the TabFact dataset. In addition to impressive overall performance, our method also has the advantage of interpretability, where the returned results are to some extent tractable with the generated sub-evidence and sub-questions.},
  comment   = {Citations - 45 (16/10/2024)},
  url       = {https://arxiv.org/pdf/2301.13808},
}

@Article{Narayan2022,
  author   = {Narayan, Avanika and Chami, Ines and Orr, Laurel and Arora, Simran and R{\'e}, Christopher},
  journal  = {arXiv preprint arXiv:2205.09911},
  title    = {Can foundation models wrangle your data?},
  year     = {2022},
  abstract = {Foundation Models (FMs) are models trained on large corpora of data that, at very large scale, can generalize to new tasks without any task-specific finetuning. As these models continue to grow in size, innovations continue to push the boundaries of what these models can do on language and image tasks. This paper aims to understand an underexplored area of FMs: classical data tasks like cleaning and integration. As a proof-of-concept, we cast five data cleaning and integration tasks as prompting tasks and evaluate the performance of FMs on these tasks. We find that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks. We identify specific research challenges and opportunities that these models present, including challenges with private and domain specific data, and opportunities to make data management systems more accessible to non-experts. We make our code and experiments publicly available at: https://github.com/HazyResearch/fm_data_tasks.},
  comment  = {Citations - 154 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2205.09911},
}

@Article{Wang2023,
  author   = {Wang, Zifeng and Gao, Chufan and Xiao, Cao and Sun, Jimeng},
  journal  = {arXiv preprint arXiv:2305.12081},
  title    = {MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement},
  year     = {2023},
  abstract = {Tabular data prediction has been employed in medical applications such as patient health risk rediction. However, existing methods usually revolve around the algorithm design while overlooking the
significance of data engineering. Medical tabular datasets frequently exhibit significant heterogeneity across different sources, with limited sample sizes per source. As such, previous predictors are often trained on manually curated small datasets that struggle to generalize across different tabular datasets during inference. This paper proposes to scale medical tabular data predictors (MediTab)
to various tabular inputs with varying features. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with distinct schema. It also aligns out-domain data with the target task using a ‚Äúlearn, annotate, and refinement‚Äù pipeline. The expanded training data then enables the pre-trained MediTab to infer for arbitrary tabular input in the domain without finetuning, resulting in significant improvements over
supervised baselines: it reaches an average ranking of 1.57 and 1.00 on 7 patient outcome prediction
datasets and 3 trial outcome prediction datasets, respectively. In addition, MediTab exhibits impressive zero-shot performances: it outperforms supervised XGBoost models by 8.9% and 17.2% on average in two prediction tasks, respectively.},
  comment  = {Citations - 4 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2305.12081},
}

@Article{Yu2023,
  author   = {Yu, Bowen and Fu, Cheng and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  journal  = {arXiv preprint arXiv:2306.16762},
  title    = {Unified language representation for question answering over text, tables, and images},
  year     = {2023},
  abstract = {When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task into a simpler textual QA problem that can be solved using three steps: retrieval, ranking, and generation, all within a language space. This idea takes advantage of the power of pre-trained language models and is implemented in a framework called Solar. Our experimental results show that Solar outperforms all existing methods by 10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different metrics. Additionally, Solar achieves the best performance on the WebQA leaderboard},
  comment  = {Citations - 13 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2306.16762},
}

@InProceedings{Hegselmann2023,
  author       = {Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  title        = {Tabllm: Few-shot classification of tabular data with large language models},
  year         = {2023},
  organization = {PMLR},
  pages        = {5549--5581},
  abstract     = {We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method‚Äôs ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting.},
  comment      = {Citations - 186 (16/10/2024)},
  url          = {https://proceedings.mlr.press/v206/hegselmann23a/hegselmann23a.pdf},
}

@InProceedings{Gong2020,
  author    = {Gong, Heng and Sun, Yawei and Feng, Xiaocheng and Qin, Bing and Bi, Wei and Liu, Xiaojiang and Liu, Ting},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  title     = {Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching},
  year      = {2020},
  pages     = {1978--1988},
  abstract  = {Although neural table-to-text models have achieved remarkable progress with the help of large-scale datasets, they suffer insufficient learning problem with limited training data. Recently, pre-trained language models show potential in few-shot learning with linguistic knowledge learnt from pretraining on large-scale corpus. However, benefiting table-to-text generation in few-shot setting with the powerful pretrained language model faces three challenges, including (1) the gap between the task‚Äôs structured input and the natural language input for pretraining language model.(2) The lack of modeling for table structure and (3) improving text fidelity with less incorrect expressions that are contradicting to the table. To address aforementioned problems, we propose TableGPT for table-to-text generation. At first, we utilize table transformation module with template to rewrite structured table in natural language as input for GPT-2. In addition, we exploit multi-task learning with two auxiliary tasks that preserve table‚Äôs structural information by reconstructing the structure from GPT-2‚Äôs representation and improving the text‚Äôs fidelity with content matching task aligning the table and information in the generated text. By experimenting on Humans, Songs and Books, three few-shot table-to-text datasets in different domains, our model outperforms existing systems on most few-shot settings.},
  comment   = {Citations - 82 (16/10/2024)},
  url       = {https://aclanthology.org/2020.coling-main.179.pdf},
}

@Article{Dinh2022,
  author   = {Dinh, Tuan and Zeng, Yuchen and Zhang, Ruisu and Lin, Ziqian and Gira, Michael and Rajput, Shashank and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
  journal  = {Advances in Neural Information Processing Systems},
  title    = {Lift: Language-interfaced fine-tuning for non-language machine learning tasks},
  year     = {2022},
  pages    = {11763--11784},
  volume   = {35},
  abstract = {Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling" no-code machine learning with LMs." We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, eg, context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github. com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.},
  comment  = {Citations - 99 (16/10/2024)},
  url      = {https://proceedings.neurips.cc/paper_files/paper/2022/file/4ce7fe1d2730f53cb3857032952cd1b8-Paper-Conference.pdf},
}

@Article{Chen2023,
  author   = {Chen, Nuo and Shou, Linjun and Gong, Ming and Pei, Jian and You, Chenyu and Chang, Jianhui and Jiang, Daxin and Li, Jia},
  journal  = {arXiv preprint arXiv:2302.09302},
  title    = {Bridge the gap between language models and tabular understanding},
  year     = {2023},
  abstract = {Table pretrain-then-finetune paradigm has been proposed and employed at a rapid pace after the success of pre-training in the natural language domain. Despite the promising findings in tabular pre-trained language models (TPLMs), there is an input gap between pre-training and fine-tuning phases. For instance, TPLMs jointly pre-trained with table and text input could be effective for tasks also with table-text joint input like table question answering, but it may fail for tasks with only tables or text as input such as table retrieval. To this end, we propose UTP, an approach that dynamically supports three types of multi-modal inputs: table-text, table, and text. Specifically, UTP is pre-trained with two strategies: (1) We first utilize a universal mask language modeling objective on each kind of input, enforcing the model to adapt various inputs. (2) We then present Cross-Modal Contrastive Regularization (CMCR), which utilizes contrastive learning to encourage the consistency between table-text cross-modality representations via unsupervised instance-wise training signals during pre-training. By these means, the resulting model not only bridges the input gap between pre-training and fine-tuning but also advances in the alignment of table and text. Extensive results show UTP achieves superior results on uni-modal input tasks (e.g., table retrieval) and cross-modal input tasks (e.g., table question answering).},
  comment  = {Citations - 4 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2302.09302},
}

@Article{Cong2023,
  author   = {Cong, Tianji and Hulsebos, Madelon and Sun, Zhenjie and Groth, Paul and Jagadish, HV},
  journal  = {arXiv preprint arXiv:2310.07736},
  title    = {Observatory: Characterizing embeddings of relational tables},
  year     = {2023},
  abstract = {Language models and specialized table embedding models have recently demonstrated strong performance on many tasks over tabular data. Researchers and practitioners are keen to leverage these models in many new application contexts; but limited understanding of the strengths and weaknesses of these models, and the table representations they generate, makes the process of finding a suitable model for a given task reliant on trial and error. There is an urgent need to gain a comprehensive understanding of these models to minimize inefficiency and failures in downstream usage. To address this need, we propose Observatory, a formal framework to systematically analyze embedding representations of relational tables. Motivated both by invariants of the relational data model and by statistical considerations regarding data distributions, we define eight primitive properties, and corresponding measures to quantitatively characterize table embeddings for these properties. Based on these properties, we define an extensible framework to evaluate language and table embedding models. We collect and synthesize a suite of datasets and use Observatory to analyze seven such models. Our analysis provides insights into the strengths and weaknesses of learned representations over tables. We find, for example, that some models are sensitive to table structure such as column order, that functional dependencies are rarely reflected in embeddings, and that specialized table embedding models have relatively lower sample fidelity. Such insights help researchers and practitioners better anticipate model behaviors and select appropriate models for their downstream tasks, while guiding researchers in the development of new models.},
  comment  = {Citations - 8 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2310.07736},
}

@Article{Sarkar2023,
  author   = {Sarkar, Soumajyoti and Lausen, Leonard},
  journal  = {arXiv preprint arXiv:2310.00789},
  title    = {Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks},
  year     = {2023},
  abstract = {Tables stored in databases and tables which are present in web pages and articles account for a large part of semi-structured data that is available on the internet. It then becomes pertinent to develop a modeling approach with large language models (LLMs) that can be used to solve diverse table tasks such as semantic parsing, question answering as well as classification problems. Traditionally, there existed separate models specialized for each task individually. It raises the question of how far can we go to build a unified model that works well on some table tasks without significant degradation on others. To that end, we attempt at creating a shared modeling approach in the pretraining stage with encoder-decoder style LLMs that can cater to diverse tasks. We evaluate our approach that continually pretrains and finetunes different model families of T5 with data from tables and surrounding context, on these downstream tasks at different model scales. Through multiple ablation studies, we observe that our pretraining with self-supervised objectives can significantly boost the performance of the models on these tasks. As an example of one improvement, we observe that the instruction finetuned public models which come specialized on text question answering (QA) and have been trained on table data still have room for improvement when it comes to table specific QA. Our work is the first attempt at studying the advantages of a unified approach to table specific pretraining when scaled from 770M to 11B sequence to sequence models while also comparing the instruction finetuned variants of the models.},
  comment  = {Citatiosn - 2 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2402.17944},
}

@Article{Zha2023,
  author   = {Zha, Liangyu and Zhou, Junlin and Li, Liyao and Wang, Rui and Huang, Qingyi and Yang, Saisai and Yuan, Jing and Su, Changbao and Li, Xiang and Su, Aofeng and others},
  journal  = {arXiv preprint arXiv:2307.08674},
  title    = {Tablegpt: Towards unifying tables, nature language and commands into one gpt},
  year     = {2023},
  abstract = {Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.},
  comment  = {Citations - 15 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2307.08674},
}

@Article{Zhang2023a,
  author   = {Zhang, Tianshu and Yue, Xiang and Li, Yifei and Sun, Huan},
  journal  = {arXiv preprint arXiv:2311.09206},
  title    = {Tablellama: Towards open large generalist models for tables},
  year     = {2023},
  abstract = {Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model's generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables.},
  comment  = {Citations - 40 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2311.09206},
}

@Article{Li2023a,
  author   = {Li, Peng and He, Yeye and Yashar, Dror and Cui, Weiwei and Ge, Song and Zhang, Haidong and Fainman, Danielle Rifinski and Zhang, Dongmei and Chaudhuri, Surajit},
  journal  = {arXiv preprint arXiv:2310.09263},
  title    = {Table-gpt: Table-tuned gpt for diverse table tasks},
  year     = {2023},
  abstract = {Language models, such as GPT-3 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models
using a range of basic table-understanding tasks, we observe that today‚Äôs language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on onedimensional natural-language texts, whereas relational tables are two-dimensional objects.
In this work, we propose a new ‚Äútable-tuning‚Äù paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as
training data, with the goal of enhancing language models‚Äô ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better table-understanding
capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout unseen tasks, and (2) strong generalizability, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.},
  comment  = {Citations - 37 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2310.09263},
}

@Article{Liu2021,
  author   = {Liu, Qian and Chen, Bei and Guo, Jiaqi and Ziyadi, Morteza and Lin, Zeqi and Chen, Weizhu and Lou, Jian-Guang},
  journal  = {arXiv preprint arXiv:2107.07653},
  title    = {TAPEX: Table pre-training via learning a neural SQL executor},
  year     = {2021},
  abstract = {Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we propose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL executor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes the improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs and to achieve new state-of-the-art results on various downstream tasks. Our code can be found at https://github.com/microsoft/Table-Pretraining.},
  comment  = {Citations - 198 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2107.07653},
}

@Article{Chang2023,
  author   = {Chang, Shuaichen and Fosler-Lussier, Eric},
  journal  = {arXiv preprint arXiv:2305.11853},
  title    = {How to prompt llms for text-to-sql: A study in zero-shot, single-domain, and cross-domain settings},
  year     = {2023},
  abstract = {Large language models (LLMs) with in-context learning have demonstrated remarkable capability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and intermediate reasoning steps to enhance the performance of LLMs. However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across various settings and provide insights into prompt constructions for future text-to-SQL studies.},
  comment  = {Citations - 35 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2305.11853},
}

@Article{Zhao2023,
  author   = {Zhao, Yilun and Zhao, Chen and Nan, Linyong and Qi, Zhenting and Zhang, Wenlin and Tang, Xiangru and Mi, Boyu and Radev, Dragomir},
  journal  = {arXiv preprint arXiv:2306.14321},
  title    = {Robut: A systematic study of table qa robustness against human-annotated adversarial perturbations},
  year     = {2023},
  abstract = {Despite significant progress having been made in question answering on tabular data (Table QA), it's unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the robustness of Table QA models, we propose a benchmark called RobuT, which builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and includes human-annotated adversarial perturbations in terms of table header, table content, and question. Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets. We propose to address this problem by using large language models to generate adversarial examples to enhance training, which significantly improves the robustness of Table QA models. Our data and code is publicly available at https://github.com/yilunzhao/RobuT.},
  comment  = {Citations - 14 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2306.14321},
}

@Article{Chen2022,
  author   = {Chen, Wenhu},
  journal  = {arXiv preprint arXiv:2210.06710},
  title    = {Large language models are few (1)-shot table reasoners},
  year     = {2022},
  abstract = {Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very strong performance with only a 1-shot demonstration, even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in https://github.com/wenhuchen/TableCoT.},
  comment  = {Citations - 100 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2210.06710},
}

@Article{Sundar2023,
  author   = {Sundar, Anirudh S and Heck, Larry},
  journal  = {arXiv preprint arXiv:2303.12024},
  title    = {cTBLS: Augmenting large language models with conversational tables},
  year     = {2023},
  abstract = {Optimizing accuracy and performance while eliminating hallucinations of open-domain conversational large language models (LLMs) is an open research challenge. A particularly promising direction is to augment and ground LLMs with information from structured sources. This paper introduces Conversational Tables (cTBLS), a three-step architecture to retrieve and generate dialogue responses grounded on retrieved tabular information. cTBLS uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 125% relative improvement over the retriever in the previous state-of-the-art system on the HyrbiDialogue dataset. cTBLS then uses a shared process between encoder and decoder models to perform a coarse+fine tabular knowledge (e.g., cell) ranking combined with a GPT-3.5 LLM response generator to yield a 2x relative improvement in ROUGE scores. Finally, human evaluators prefer cTBLs +80% of the time (coherency, fluency) and judge informativeness to be 4x better than the previous state-of-the-art.},
  comment  = {Citations - 10 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2303.12024},
}

@InProceedings{Liu2023a,
  author    = {Liu, Shang-Ching and Wang, ShengKun and Chang, Tsungyao and Lin, Wenqi and Hsiung, Chung-Wei and Hsieh, Yi-Chen and Cheng, Yu-Ping and Luo, Sian-Hong and Zhang, Jianwei},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  title     = {JarviX: A LLM no code platform for tabular data analysis and optimization},
  year      = {2023},
  pages     = {622--630},
  abstract  = {In this study, we introduce JarviX, a sophisticated data analytics framework. JarviX is designed to employ Large Language Models (LLMs) to facilitate an automated guide and execute high-precision data analyzes on tabular datasets. This framework emphasizes the significance of varying column types, capitalizing on state-of-the-art LLMs to generate concise data insight summaries, propose relevant analysis inquiries, visualize data effectively, and provide comprehensive explanations for results drawn from an extensive data analysis pipeline. Moreover, JarviX incorporates an automated machine learning (AutoML) pipeline for predictive modeling. This integration forms a comprehensive and automated optimization cycle, which proves particularly advantageous for optimizing machine configuration. The efficacy and adaptability of JarviX are substantiated through a series of practical use case studies.},
  comment   = {Citations - 6 (16/10/2024)},
  url       = {https://aclanthology.org/2023.emnlp-industry.59.pdf},
}

@Article{Lobo2023,
  author   = {Lobo, Elita and Hassanzadeh, Oktie and Pham, Nhan and Mihindukulasooriya, Nandana and Subramanian, Dharmashankar and Samulowitz, Horst},
  journal  = {arXiv preprint arXiv:2309.11506},
  title    = {Matching Table Metadata with Business Glossaries Using Large Language Models},
  year     = {2023},
  abstract = {Enterprises often own large collections of structured data in the form of large databases or an enterprise data lake. Such data collections come with limited metadata and strict access policies that could limit access to the data contents and, therefore, limit the application of classic retrieval and analysis solutions. As a result, there is a need for solutions that can effectively utilize the available metadata. In this paper, we study the problem of matching table metadata to a business glossary containing data labels and descriptions. The resulting matching enables the use of an available or curated business glossary for retrieval and analysis without or before requesting access to the data contents. One solution to this problem is to use manually-defined rules or similarity measures on column names and glossary descriptions (or their vector embeddings) to find the closest match. However, such approaches need to be tuned through manual labeling and cannot handle many business glossaries that contain a combination of simple as well as complex and long descriptions. In this work, we leverage the power of large language models (LLMs) to design generic matching methods that do not require manual tuning and can identify complex relations between column names and glossaries. We propose methods that utilize LLMs in two ways: a) by generating additional context for column names that can aid with matching b) by using LLMs to directly infer if there is a relation between column names and glossary descriptions. Our preliminary experimental results show the effectiveness of our proposed methods.},
  comment  = {Citations - 2 (17/10/2024)},
  url      = {https://arxiv.org/pdf/2309.11506},
}

@Article{Fernandez2023,
  author    = {Fernandez, Raul Castro and Elmore, Aaron J and Franklin, Michael J and Krishnan, Sanjay and Tan, Chenhao},
  journal   = {Proceedings of the VLDB Endowment},
  title     = {How large language models will disrupt data management},
  year      = {2023},
  number    = {11},
  pages     = {3302--3309},
  volume    = {16},
  abstract  = {Large language models (LLMs), such as GPT-4, are revolutionizing software's ability to understand, process, and synthesize language. The authors of this paper believe that this advance in technology is significant enough to prompt introspection in the data management community, similar to previous technological disruptions such as the advents of the world wide web, cloud computing, and statistical machine learning. We argue that the disruptive influence that LLMs will have on data management will come from two angles. (1) A number of hard database problems, namely, entity resolution, schema matching, data discovery, and query synthesis, hit a ceiling of automation because the system does not fully understand the semantics of the underlying data. Based on large training corpora of natural language, structured data, and code, LLMs have an unprecedented ability to ground database tuples, schemas, and queries in real-world concepts. We will provide examples of how LLMs may completely change our approaches to these problems. (2) LLMs blur the line between predictive models and information retrieval systems with their ability to answer questions. We will present examples showing how large databases and information retrieval systems have complementary functionality.},
  comment   = {Citations - 57 (17/10/2024)},
  publisher = {VLDB Endowment},
  url       = {https://raulcastrofernandez.com/papers/llm_db_vision_vldb23-11.pdf},
}

@Article{Zhang2023b,
  author   = {Zhang, Haochen and Dong, Yuyang and Xiao, Chuan and Oyamada, Masafumi},
  journal  = {arXiv preprint arXiv:2312.01678},
  title    = {Jellyfish: A large language model for data preprocessing},
  year     = {2023},
  abstract = {In this paper, we present Jellyfish, an open-source LLM as a universal task solver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tuned with the datasets of several typical DP tasks including error detection, data imputation, schema matching, and entity matching, and delivers generalizability to other tasks. Remarkably, Jellyfish can operate on a local, single, and low-priced GPU with its 13 billion parameters, ensuring data security and enabling further tuning. Its proficiency in understanding natural language allows users to manually craft instructions for DP tasks. Unlike many existing methods that heavily rely on prior knowledge, Jellyfish acquires domain knowledge during its tuning process and integrates optional knowledge injection during inference. A distinctive feature of Jellyfish is its interpreter, which elucidates its output decisions. To construct Jellyfish, we develop a series of pre-tuning and DP-tuning techniques. Jellyfish is equipped with an instance serializer, which automatically translates raw data into model prompts, and a knowledge injector, which optionally introduces task- and dataset-specific knowledge to enhance DP performance. Our evaluation of Jellyfish, using a range of real datasets, shows its competitiveness compared to state-of-the-art methods and its strong generalizability to unseen tasks. Jellyfish's performance rivals that of GPT series models, and its interpreter offers enhanced reasoning capabilities compared to GPT-3.5. Furthermore, our evaluation highlights the effectiveness of the techniques employed in constructing Jellyfish. Our model is available at Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish .},
  comment  = {Citations -14 (17/10/2024)},
  url      = {https://arxiv.org/abs/2312.01678},
}

@Article{Nan2022,
  author    = {Nan, Linyong and Hsieh, Chiachun and Mao, Ziming and Lin, Xi Victoria and Verma, Neha and Zhang, Rui and Kry{\'s}ci{\'n}ski, Wojciech and Schoelkopf, Hailey and Kong, Riley and Tang, Xiangru and others},
  journal   = {Transactions of the Association for Computational Linguistics},
  title     = {FeTaQA: Free-form table question answering},
  year      = {2022},
  pages     = {35--49},
  volume    = {10},
  abstract  = {Existing table question answering datasets contain abundant factual questions that primarily evaluate a QA system‚Äôs comprehension of query and tabular data. However, restricted by their short-form answers, these datasets fail to include question‚Äìanswer interactions that represent more advanced and naturally occurring information needs: questions that ask for reasoning and integration of information pieces retrieved from a structured knowledge source. To complement the existing datasets and to reveal the challenging nature of the table-based question answering task, we introduce FeTaQA, a new dataset with 10K Wikipedia-based {table, question, free-form answer, supporting table cells} pairs. FeTaQA is collected from noteworthy descriptions of Wikipedia tables that contain information people tend to seek; generation of these descriptions requires advanced processing that humans perform on a daily basis: Understand the question and table, retrieve, integrate, infer, and conduct text planning and surface realization to generate an answer. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based QA systems and an end-to-end method based on large pretrained text generation models, and show that FeTaQA poses a challenge for both methods.},
  comment   = {Citations - 34 (17/10/2024)},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~‚Ä¶},
  url       = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00446/2012156/tacl_a_00446.pdf},
}

@Article{Zhang2024a,
  author   = {Zhang, Siyue and Luu, Anh Tuan and Zhao, Chen},
  journal  = {arXiv preprint arXiv:2409.16682},
  title    = {SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA},
  year     = {2024},
  abstract = {Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main approaches for Table-based Question Answering task. Despite success on multiple benchmarks, they have yet to be compared and their synergy remains unexplored. In this paper, we identify different strengths and weaknesses through evaluating state-of-the-art models on benchmark datasets: Text-to-SQL demonstrates superiority in handling questions involving arithmetic operations and long tables; E2E TQA excels in addressing ambiguous questions, non-standard table schema, and complex table contents. To combine both strengths, we propose a Synergistic Table-based Question Answering approach that integrate different models via answer selection, which is agnostic to any model types. Further experiments validate that ensembling models by either feature-based or LLM-based answer selector significantly improves the performance over individual models.},
  comment  = {Citations - 0 (17/10/2024)},
  url      = {https://arxiv.org/pdf/2409.16682},
}

@Article{Chen2020,
  author   = {Chen, Wenhu and Zha, Hanwen and Chen, Zhiyu and Xiong, Wenhan and Wang, Hong and Wang, William},
  journal  = {arXiv preprint arXiv:2004.07347},
  title    = {Hybridqa: A dataset of multi-hop question answering over tabular and textual data},
  year     = {2020},
  abstract = {Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA https://github.com/wenhuchen/HybridQA, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20\%, while the hybrid model can achieve an EM over 40\%. This gap suggests the necessity to aggregate heterogeneous information in HybridQA. However, the hybrid model's score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information.},
  comment  = {Citations - 285 (17/10/2024)},
  url      = {https://arxiv.org/pdf/2004.07347},
}

@InProceedings{Iyyer2017,
  author    = {Iyyer, Mohit and Yih, Wen-tau and Chang, Ming-Wei},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Search-based neural structured learning for sequential question answering},
  year      = {2017},
  pages     = {1821--1831},
  abstract  = {Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search. Our model effectively leverages the sequential context to outperform state-of-the-art QA systems that are designed to answer highly complex questions.},
  comment   = {Citations - 241 (17/10/2024)},
  url       = {https://aclanthology.org/P17-1167.pdf},
}

@Article{Cheng2021,
  author   = {Cheng, Zhoujun and Dong, Haoyu and Wang, Zhiruo and Jia, Ran and Guo, Jiaqi and Gao, Yan and Han, Shi and Lou, Jian-Guang and Zhang, Dongmei},
  journal  = {arXiv preprint arXiv:2108.06712},
  title    = {Hitab: A hierarchical table dataset for question answering and natural language generation},
  year     = {2021},
  abstract = {Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge existing methods by hierarchical indexing, as well as implicit relationships of calculation and semantics. This work presents HiTab, a free and open dataset to study question answering (QA) and natural language generation (NLG) over hierarchical tables. HiTab is a cross-domain dataset constructed from a wealth of statistical reports (analyses) and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) both target sentences for NLG and questions for QA are revised from original, meaningful, and diverse descriptive sentences authored by analysts and professions of reports. (3) to reveal complex numerical reasoning in statistical analyses, we provide fine-grained annotations of entity and quantity alignment. HiTab provides 10,686 QA pairs and descriptive sentences with well-annotated quantity and entity alignment on 3,597 tables with broad coverage of table hierarchies and numerical reasoning types. Targeting hierarchical structure, we devise a novel hierarchy-aware logical form for symbolic reasoning over tables, which shows high effectiveness. Targeting complex numerical reasoning, we propose partially supervised training given annotations of entity and quantity alignment, which helps models to largely reduce spurious predictions in the QA task. In the NLG task, we find that entity and quantity alignment also helps NLG models to generate better results in a conditional generation setting. Experiment results of state-of-the-art baselines suggest that this dataset presents a strong challenge and a valuable benchmark for future research.},
  comment  = {Citations - 71 (16/10/2024)},
  url      = {https://arxiv.org/pdf/2108.06712},
}

@InProceedings{Parikh2020,
  author    = {Parikh, Ankur P and Wang, Xuezhi and Gehrmann, Sebastian and Faruqui, Manaal and Dhingra, Bhuwan and Yang, Diyi and Das, Dipanjan},
  booktitle = {Proceedings of EMNLP},
  title     = {{ToTTo}: A Controlled Table-To-Text Generation Dataset},
  year      = {2020},
  abstract  = {We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.},
  comment   = {Citations - 344 (17/10/2024)},
  url       = {https://arxiv.org/abs/2004.14373},
}

@Article{Aly2021,
  author   = {Aly, Rami and Guo, Zhijiang and Schlichtkrull, Michael and Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Cocarascu, Oana and Mittal, Arpit},
  journal  = {arXiv preprint arXiv:2106.05707},
  title    = {Feverous: Fact extraction and verification over unstructured and structured information},
  year     = {2021},
  abstract = {Fact verification has attracted a lot of attention in the machine learning and natural language processing communities, as it is one of the key methods for detecting misinformation. Existing large-scale benchmarks for this task have focused mostly on textual sources, i.e. unstructured information, and thus ignored the wealth of information available in structured formats, such as tables. In this paper we introduce a novel dataset and benchmark, Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict. Furthermore, we detail our efforts to track and minimize the biases present in the dataset and could be exploited by models, e.g. being able to predict the label without using evidence. Finally, we develop a baseline for verifying claims against text and tables which predicts both the correct evidence and verdict for 18% of the claims.},
  comment  = {Citations - 145 (17/10/2024)},
  url      = {https://assets.amazon.science/23/e2/fbdef0254679b6022614b222c30a/feverous-fact-extraction-and-verification-over-unstructured-and-structured-information.pdf},
}

@Article{Chen2019,
  author   = {Chen, Wenhu and Wang, Hongmin and Chen, Jianshu and Zhang, Yunkai and Wang, Hong and Li, Shiyang and Zhou, Xiyou and Wang, William Yang},
  journal  = {arXiv preprint arXiv:1909.02164},
  title    = {Tabfact: A large-scale dataset for table-based fact verification},
  year     = {2019},
  abstract = {The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables
and statements into continuous vectors for verification. LPA parses statements into programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities. The data and code of the dataset are provided in
https://github.com/wenhuchen/Table-Fact-Checking.},
  comment  = {Citations - 414 (17/10/2024)},
  url      = {https://arxiv.org/pdf/1909.02164},
}

@Article{Yu2018a,
  author   = {Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},
  journal  = {arXiv preprint arXiv:1809.08887},
  title    = {Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task},
  year     = {2018},
  abstract = {We present Spider, a large-scale, complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task where different complex SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and the exact same programs in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 14.3% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task are publicly available at https://yale-lily.github.io/spider.},
  comment  = {Citations - 1056 (17/10/2024)},
  url      = {https://arxiv.org/pdf/1809.08887},
}

@Article{Gupta2020,
  author   = {Gupta, Vivek and Mehta, Maitrey and Nokhiz, Pegah and Srikumar, Vivek},
  journal  = {arXiv preprint arXiv:2005.06117},
  title    = {INFOTABS: Inference on tables as semi-structured data},
  year     = {2020},
  abstract = {In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. We argue that such data can prove as a testing ground for understanding how we reason about information. To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes. Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning. Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at
the task, suggesting that reasoning about tables can pose a difficult modeling challenge.},
  comment  = {Citations - 94 (17/10/2024)},
  url      = {https://arxiv.org/pdf/2005.06117},
}

@Article{Katsis2021,
  author   = {Katsis, Yannis and Chemmengath, Saneem and Kumar, Vishwajeet and Bharadwaj, Samarth and Canim, Mustafa and Glass, Michael and Gliozzo, Alfio and Pan, Feifei and Sen, Jaydeep and Sankaranarayanan, Karthik and others},
  journal  = {arXiv preprint arXiv:2106.12944},
  title    = {AIT-QA: Question answering dataset over complex tables in the airline industry},
  year     = {2021},
  abstract = {Recent advances in transformers have enabled Table Question Answering (Table QA) systems to achieve high accuracy and SOTA results on open domain datasets like WikiTableQuestions and WikiSQL. Such transformers are frequently pre-trained on open-domain content such as Wikipedia, where they effectively encode questions and corresponding tables from Wikipedia as seen in Table QA dataset. However, web tables in Wikipedia are notably flat in their layout, with the first row as the sole column header. The layout lends to a relational view of tables where each row is a tuple. Whereas, tables in domain-specific business or scientific documents often have a much more complex layout, including hierarchical row and column headers, in addition to having specialized vocabulary terms from that domain. To address this problem, we introduce the domain-specific Table QA dataset AIT-QA (Airline Industry Table QA). The dataset consists of 515 questions authored by human annotators on 116 tables extracted from public U.S. SEC filings (publicly available at: https://www.sec.gov/edgar.shtml) of major airline companies for the fiscal years 2017-2019. We also provide annotations pertaining to the nature of questions, marking those that require hierarchical headers, domain-specific terminology, and paraphrased forms. Our zero-shot baseline evaluation of three transformer-based SOTA Table QA methods - TaPAS (end-to-end), TaBERT (semantic parsing-based), and RCI (row-column encoding-based) - clearly exposes the limitation of these methods in this practical setting, with the best accuracy at just 51.8\% (RCI). We also present pragmatic table preprocessing steps used to pivot and project these complex tables into a layout suitable for the SOTA Table QA models.},
  comment  = {Citations - 33 (17/10/2024)},
  url      = {https://arxiv.org/pdf/2106.12944},
}

@Article{Zhu2021,
  author   = {Zhu, Fengbin and Lei, Wenqiang and Huang, Youcheng and Wang, Chao and Zhang, Shuo and Lv, Jiancheng and Feng, Fuli and Chua, Tat-Seng},
  journal  = {arXiv preprint arXiv:2105.07624},
  title    = {TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance},
  year     = {2021},
  abstract = {Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and the compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. TAGOPachieves 58.0% inF1, which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind performance of expert human, i.e.90.8% in F1. It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data.},
  comment  = {Citations - 207 (17/10/2024)},
  url      = {https://arxiv.org/pdf/2105.07624},
}

@Article{Moosavi2021,
  author   = {Moosavi, Nafise Sadat and R{\"u}ckl{\'e}, Andreas and Roth, Dan and Gurevych, Iryna},
  journal  = {arXiv preprint arXiv:2104.08296},
  title    = {Learning to reason for text generation from scientific tables},
  year     = {2021},
  abstract = {In this paper, we introduce SciGen, a new challenge dataset for the task of reasoningaware data-to-text generation consisting of tables from scientific articles and their corresponding descriptions. Describing scientific tables goes beyond the surface realization of
the table content and requires reasoning over table values. The unique properties of SciGen are that (1) tables mostly contain numerical values, and (2) the corresponding descriptions require arithmetic reasoning. SciGen is therefore the first dataset that assesses the arithmetic reasoning capabilities of generation models on complex input structures, i.e., tables from scientific articles. We study the effectiveness of state-of-the-art data-to-text generation models on SciGen and evaluate the results using common metrics as well as human evaluation. Our results and analyses show that (a) while humans like to reason for describing scientific tables, the ability of stateof-the-art models is severely limited on this task, (b) while adding more training data improves the results, it is not the solution for
reasoning-aware text generation, and (c) one of the main bottlenecks for this task is the lack
of proper automatic evaluation metrics. The data, code, and annotations for human evaluation will be available at https://github.com/UKPLab/SciGen. SciGen opens new avenues for future research in reasoning-aware text generation and evaluation.},
  comment  = {Citations - 17 (17/10/2024)},
  url      = {https://arxiv.org/pdf/2104.08296},
}

@Article{Yin2018,
  author   = {Yin, Pengcheng and Zhou, Chunting and He, Junxian and Neubig, Graham},
  journal  = {arXiv preprint arXiv:1806.07832},
  title    = {Structvae: Tree-structured latent variable models for semi-supervised semantic parsing},
  year     = {2018},
  abstract = {Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce StructVAE, a variational auto-encoding model for semisupervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.},
  comment  = {Citations - 122 (17/10/2024)},
  url      = {https://arxiv.org/pdf/1806.07832},
}

@Comment{jabref-meta: databaseType:bibtex;}
